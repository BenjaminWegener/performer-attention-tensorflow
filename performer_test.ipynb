{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "from collections.abc import Iterable  # pylint: disable=g-importing-member\n",
    "import functools\n",
    "from absl import logging\n",
    "import gin\n",
    "import jax\n",
    "from jax import lax\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import numpy as onp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 (2, 512, 16, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.77132064, 0.02075195, 0.6336482 , ..., 0.90283173, 0.53455794,\n",
       "        0.5902014 ],\n",
       "       [0.03928177, 0.35718176, 0.07961309, ..., 0.5982557 , 0.14762019,\n",
       "        0.18403482],\n",
       "       [0.6450721 , 0.04862801, 0.24861251, ..., 0.33867913, 0.09465904,\n",
       "        0.71583086],\n",
       "       ...,\n",
       "       [0.9215931 , 0.15262388, 0.74999446, ..., 0.42338467, 0.7379784 ,\n",
       "        0.9901201 ],\n",
       "       [0.06883631, 0.85244584, 0.78474957, ..., 0.9454265 , 0.41213632,\n",
       "        0.3258093 ],\n",
       "       [0.3214651 , 0.2492156 , 0.7389259 , ..., 0.4773599 , 0.33000788,\n",
       "        0.24054141]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (2, 512, 16, 64)\n",
    "onp.random.seed(10)\n",
    "dtype = onp.float32\n",
    "query = onp.random.rand(*shape).astype(dtype)  #(bs, len, num_heads, head_dim)\n",
    "key = onp.random.rand(*shape).astype(dtype)\n",
    "value = onp.random.rand(*shape).astype(dtype)\n",
    "# loc = 0.0\n",
    "# scale = 1.0\n",
    "# dtype = onp.float32\n",
    "# query = onp.random.normal(loc, scale, size=shape).astype(dtype)  #(bs, len, num_heads, head_dim)\n",
    "# key = onp.random.normal(loc, scale, size=shape).astype(dtype)\n",
    "# value = onp.random.normal(loc, scale, size=shape).astype(dtype)\n",
    "\n",
    "print(query.dtype, query.shape)\n",
    "query[0,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2020 The Google Research Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Core Fast Attention Module for Flax.\n",
    "Implementation of the approximate fast softmax and generalized\n",
    "attention mechanism leveraging structured random feature maps [RFM] techniques\n",
    "and low rank decomposition of the attention matrix.\n",
    "\"\"\"\n",
    "# pylint: disable=invalid-name, missing-function-docstring\n",
    "\n",
    "import abc\n",
    "from collections.abc import Iterable  # pylint: disable=g-importing-member\n",
    "import functools\n",
    "from absl import logging\n",
    "import gin\n",
    "import jax\n",
    "from jax import lax\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import numpy as onp\n",
    "\n",
    "# Nonlinear mappings encoding different attention kernels.\n",
    "gin.external_configurable(jnp.cos, 'jcos')\n",
    "gin.external_configurable(jnp.sin, 'jsin')\n",
    "gin.external_configurable(jnp.tanh, 'jtanh')\n",
    "gin.external_configurable(jax.nn.sigmoid, 'jsigmoid')\n",
    "gin.external_configurable(jax.nn.relu, 'jrelu')\n",
    "gin.external_configurable(lambda x: x * x * (x > 0.0), 'jrequ')\n",
    "gin.external_configurable(jax.nn.gelu, 'jgelu')\n",
    "gin.external_configurable(jnp.exp, 'jexp')\n",
    "gin.external_configurable(lambda x: x, 'jidentity')\n",
    "\n",
    "\n",
    "def nonnegative_softmax_kernel_feature_creator(data,\n",
    "                                               projection_matrix,\n",
    "                                               attention_dims_t,\n",
    "                                               batch_dims_t,\n",
    "                                               precision,\n",
    "                                               is_query,\n",
    "                                               normalize_data=True,\n",
    "                                               eps=0.0001):\n",
    "  \"\"\"Constructs nonnegative kernel features for fast softmax attention.\n",
    "  Args:\n",
    "    data: input for which features are computes\n",
    "    projection_matrix: random matrix used to compute features\n",
    "    attention_dims_t: tuple of attention dimensions\n",
    "    batch_dims_t: tuple of batch dimensions\n",
    "    precision: precision parameter\n",
    "    is_query: predicate indicating whether input data corresponds to queries or\n",
    "      keys\n",
    "    normalize_data: predicate indicating whether data should be normalized,\n",
    "    eps: numerical stabilizer.\n",
    "  Returns:\n",
    "    Random features for fast softmax attention.\n",
    "  \"\"\"\n",
    "  del attention_dims_t\n",
    "  if normalize_data:\n",
    "    # We have e^{qk^T/sqrt{d}} = e^{q_norm k_norm^T}, where\n",
    "    # w_norm = w * data_normalizer for w in {q,k}.\n",
    "    data_normalizer = 1.0 / (jnp.sqrt(jnp.sqrt(data.shape[-1])))\n",
    "  else:\n",
    "    data_normalizer = 1.0\n",
    "  ratio = 1.0 / jnp.sqrt(projection_matrix.shape[0])\n",
    "  data_mod_shape = data.shape[0:len(batch_dims_t)] + projection_matrix.shape\n",
    "  data_thick_random_matrix = jnp.zeros(data_mod_shape) + projection_matrix\n",
    "\n",
    "  data_dash = lax.dot_general(\n",
    "      data_normalizer * data,\n",
    "      data_thick_random_matrix,\n",
    "      (((data.ndim - 1,), (data_thick_random_matrix.ndim - 1,)),\n",
    "       (batch_dims_t, batch_dims_t)),\n",
    "      precision=precision)\n",
    "\n",
    "  diag_data = jnp.square(data)\n",
    "  diag_data = jnp.sum(diag_data, axis=data.ndim - 1)\n",
    "  diag_data = (diag_data / 2.0) * data_normalizer * data_normalizer\n",
    "  diag_data = jnp.expand_dims(diag_data, axis=data.ndim - 1)\n",
    "\n",
    "  if is_query:\n",
    "    last_dims_t = (len(data_dash.shape) - 1,)\n",
    "    data_dash = ratio * (\n",
    "        jnp.exp(data_dash - diag_data -\n",
    "                jnp.max(data_dash, axis=last_dims_t, keepdims=True)) + eps)\n",
    "  else:\n",
    "    data_dash = ratio * (\n",
    "        jnp.exp(data_dash - diag_data - jnp.max(data_dash)) + eps)\n",
    "\n",
    "  return data_dash\n",
    "\n",
    "\n",
    "def sincos_softmax_kernel_feature_creator(data,\n",
    "                                          projection_matrix,\n",
    "                                          attention_dims_t,\n",
    "                                          batch_dims_t,\n",
    "                                          precision,\n",
    "                                          normalize_data=True):\n",
    "  \"\"\"Constructs kernel sin-cos features for fast softmax attention.\n",
    "  Args:\n",
    "    data: input for which features are computes\n",
    "    projection_matrix: random matrix used to compute features\n",
    "    attention_dims_t: tuple of attention dimensions\n",
    "    batch_dims_t: tuple of batch dimensions\n",
    "    precision: precision parameter\n",
    "    normalize_data: predicate indicating whether data should be normalized.\n",
    "  Returns:\n",
    "    Random features for fast softmax attention.\n",
    "  \"\"\"\n",
    "  if normalize_data:\n",
    "    # We have: exp(qk^T/sqrt{d}) = exp(|q|^2/2sqrt{d}) * exp(|k|^2/2sqrt{d}) *\n",
    "    # exp(-(|q*c-k*c|^2)/2), where c = 1.0 / sqrt{sqrt{d}}.\n",
    "    data_normalizer = 1.0 / (jnp.sqrt(jnp.sqrt(data.shape[-1])))\n",
    "  else:\n",
    "    data_normalizer = 1.0\n",
    "  ratio = 1.0 / jnp.sqrt(projection_matrix.shape[0])\n",
    "  data_mod_shape = data.shape[0:len(batch_dims_t)] + projection_matrix.shape\n",
    "  data_thick_random_matrix = jnp.zeros(data_mod_shape) + projection_matrix\n",
    "\n",
    "  data_dash = lax.dot_general(\n",
    "      data_normalizer * data,\n",
    "      data_thick_random_matrix,\n",
    "      (((data.ndim - 1,), (data_thick_random_matrix.ndim - 1,)),\n",
    "       (batch_dims_t, batch_dims_t)),\n",
    "      precision=precision)\n",
    "  data_dash_cos = ratio * jnp.cos(data_dash)\n",
    "  data_dash_sin = ratio * jnp.sin(data_dash)\n",
    "  data_dash = jnp.concatenate((data_dash_cos, data_dash_sin), axis=-1)\n",
    "\n",
    "  # Constructing D_data and data^{'}\n",
    "  diag_data = jnp.square(data)\n",
    "  diag_data = jnp.sum(diag_data, axis=data.ndim - 1)\n",
    "  diag_data = (diag_data / 2.0) * data_normalizer * data_normalizer\n",
    "  diag_data = jnp.expand_dims(diag_data, axis=data.ndim - 1)\n",
    "  # Additional renormalization for numerical stability\n",
    "  data_renormalizer = jnp.max(diag_data, attention_dims_t, keepdims=True)\n",
    "  diag_data -= data_renormalizer\n",
    "  diag_data = jnp.exp(diag_data)\n",
    "  data_prime = data_dash * diag_data\n",
    "  return data_prime\n",
    "\n",
    "\n",
    "def generalized_kernel_feature_creator(data, projection_matrix, batch_dims_t,\n",
    "                                       precision, kernel_fn, kernel_epsilon,\n",
    "                                       normalize_data):\n",
    "  \"\"\"Constructs kernel features for fast generalized attention.\n",
    "  Args:\n",
    "    data: input for which features are computes\n",
    "    projection_matrix: matrix used to compute features\n",
    "    batch_dims_t: tuple of batch dimensions\n",
    "    precision: precision parameter\n",
    "    kernel_fn: kernel function used\n",
    "    kernel_epsilon: additive positive term added to every feature for numerical\n",
    "      stability\n",
    "    normalize_data: predicate indicating whether data should be normalized.\n",
    "  Returns:\n",
    "    Random features for fast generalized attention.\n",
    "  \"\"\"\n",
    "  if normalize_data:\n",
    "    data_normalizer = 1.0 / (jnp.sqrt(jnp.sqrt(data.shape[-1])))\n",
    "  else:\n",
    "    data_normalizer = 1.0\n",
    "  if projection_matrix is None:\n",
    "    return kernel_fn(data_normalizer * data) + kernel_epsilon\n",
    "  else:\n",
    "    data_mod_shape = data.shape[0:len(batch_dims_t)] + projection_matrix.shape\n",
    "    data_thick_random_matrix = jnp.zeros(data_mod_shape) + projection_matrix\n",
    "    data_dash = lax.dot_general(\n",
    "        data_normalizer * data,\n",
    "        data_thick_random_matrix,\n",
    "        (((data.ndim - 1,), (data_thick_random_matrix.ndim - 1,)),\n",
    "         (batch_dims_t, batch_dims_t)),\n",
    "        precision=precision)\n",
    "  data_prime = kernel_fn(data_dash) + kernel_epsilon\n",
    "  return data_prime\n",
    "\n",
    "\n",
    "@gin.configurable\n",
    "def make_fast_softmax_attention(qkv_dim,\n",
    "                                renormalize_attention=True,\n",
    "                                numerical_stabilizer=0.000001,\n",
    "                                nb_features=256,\n",
    "                                ortho_features=True,\n",
    "                                ortho_scaling=0.0,\n",
    "                                redraw_features=True,\n",
    "                                unidirectional=False,\n",
    "                                nonnegative_features=True,\n",
    "                                lax_scan_unroll=1):\n",
    "  \"\"\"Construct a fast softmax attention method.\"\"\"\n",
    "  logging.info(\n",
    "      'Fast softmax attention: %s features and orthogonal=%s, renormalize=%s',\n",
    "      nb_features, ortho_features, renormalize_attention)\n",
    "  if ortho_features:\n",
    "    matrix_creator = functools.partial(\n",
    "        GaussianOrthogonalRandomMatrix,\n",
    "        nb_features,\n",
    "        qkv_dim,\n",
    "        scaling=ortho_scaling)\n",
    "  else:\n",
    "    matrix_creator = functools.partial(GaussianUnstructuredRandomMatrix,\n",
    "                                       nb_features, qkv_dim)\n",
    "  if nonnegative_features:\n",
    "\n",
    "    def kernel_feature_creator(data,\n",
    "                               projection_matrix,\n",
    "                               attention_dims_t,\n",
    "                               batch_dims_t,\n",
    "                               precision,\n",
    "                               is_query,\n",
    "                               normalize_data=True):\n",
    "      return nonnegative_softmax_kernel_feature_creator(\n",
    "          data, projection_matrix, attention_dims_t, batch_dims_t, precision,\n",
    "          is_query, normalize_data, numerical_stabilizer)\n",
    "  else:\n",
    "\n",
    "    def kernel_feature_creator(data,\n",
    "                               projection_matrix,\n",
    "                               attention_dims_t,\n",
    "                               batch_dims_t,\n",
    "                               precision,\n",
    "                               is_query,\n",
    "                               normalize_data=True):\n",
    "      del is_query\n",
    "      return sincos_softmax_kernel_feature_creator(data, projection_matrix,\n",
    "                                                   attention_dims_t,\n",
    "                                                   batch_dims_t, precision,\n",
    "                                                   normalize_data)\n",
    "\n",
    "  attention_fn = FastAttentionviaLowRankDecomposition(\n",
    "      matrix_creator,\n",
    "      kernel_feature_creator,\n",
    "      renormalize_attention=renormalize_attention,\n",
    "      numerical_stabilizer=numerical_stabilizer,\n",
    "      redraw_features=redraw_features,\n",
    "      unidirectional=unidirectional,\n",
    "      lax_scan_unroll=lax_scan_unroll).dot_product_attention\n",
    "  return attention_fn\n",
    "\n",
    "\n",
    "@gin.configurable\n",
    "def make_fast_generalized_attention(qkv_dim,\n",
    "                                    renormalize_attention=True,\n",
    "                                    numerical_stabilizer=0.0,\n",
    "                                    nb_features=256,\n",
    "                                    features_type='deterministic',\n",
    "                                    kernel_fn=jax.nn.relu,\n",
    "                                    kernel_epsilon=0.001,\n",
    "                                    redraw_features=False,\n",
    "                                    unidirectional=False,\n",
    "                                    lax_scan_unroll=1):\n",
    "  \"\"\"Construct a fast generalized attention menthod.\"\"\"\n",
    "  logging.info('Fast generalized attention.: %s features and renormalize=%s',\n",
    "               nb_features, renormalize_attention)\n",
    "  if features_type == 'ortho':\n",
    "    matrix_creator = functools.partial(\n",
    "        GaussianOrthogonalRandomMatrix, nb_features, qkv_dim, scaling=False)\n",
    "  elif features_type == 'iid':\n",
    "    matrix_creator = functools.partial(GaussianUnstructuredRandomMatrix,\n",
    "                                       nb_features, qkv_dim)\n",
    "  elif features_type == 'deterministic':\n",
    "    matrix_creator = None\n",
    "  else:\n",
    "    raise ValueError('Unknown feature value type')\n",
    "\n",
    "  def kernel_feature_creator(data,\n",
    "                             projection_matrix,\n",
    "                             attention_dims_t,\n",
    "                             batch_dims_t,\n",
    "                             precision,\n",
    "                             is_query,\n",
    "                             normalize_data=False):\n",
    "    del attention_dims_t\n",
    "    del is_query\n",
    "    return generalized_kernel_feature_creator(data, projection_matrix,\n",
    "                                              batch_dims_t, precision,\n",
    "                                              kernel_fn, kernel_epsilon,\n",
    "                                              normalize_data)\n",
    "\n",
    "  attention_fn = FastAttentionviaLowRankDecomposition(\n",
    "      matrix_creator,\n",
    "      kernel_feature_creator,\n",
    "      renormalize_attention=renormalize_attention,\n",
    "      numerical_stabilizer=numerical_stabilizer,\n",
    "      redraw_features=redraw_features,\n",
    "      unidirectional=unidirectional,\n",
    "      lax_scan_unroll=lax_scan_unroll).dot_product_attention\n",
    "  return attention_fn\n",
    "\n",
    "\n",
    "class RandomMatrix(object):\n",
    "  r\"\"\"Abstract class providing a method for constructing 2D random arrays.\n",
    "  Class is responsible for constructing 2D random arrays.\n",
    "  \"\"\"\n",
    "\n",
    "  __metaclass__ = abc.ABCMeta\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def get_2d_array(self):\n",
    "    raise NotImplementedError('Abstract method')\n",
    "\n",
    "\n",
    "class GaussianUnstructuredRandomMatrix(RandomMatrix):\n",
    "\n",
    "  def __init__(self, nb_rows, nb_columns, key):\n",
    "    self.nb_rows = nb_rows\n",
    "    self.nb_columns = nb_columns\n",
    "    self.key = key\n",
    "\n",
    "  def get_2d_array(self):\n",
    "    return random.normal(self.key, (self.nb_rows, self.nb_columns))\n",
    "\n",
    "\n",
    "class GaussianOrthogonalRandomMatrix(RandomMatrix):\n",
    "  r\"\"\"Class providing a method to create Gaussian orthogonal matrix.\n",
    "  Class is responsible for constructing 2D Gaussian orthogonal arrays.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, nb_rows, nb_columns, key, scaling=0):\n",
    "    self.nb_rows = nb_rows\n",
    "    self.nb_columns = nb_columns\n",
    "    self.key = key\n",
    "    self.scaling = scaling\n",
    "\n",
    "  def get_2d_array(self):\n",
    "    nb_full_blocks = int(self.nb_rows / self.nb_columns)\n",
    "    block_list = []\n",
    "    rng = self.key\n",
    "    for _ in range(nb_full_blocks):\n",
    "      rng, rng_input = jax.random.split(rng)\n",
    "      unstructured_block = random.normal(rng_input,\n",
    "                                         (self.nb_columns, self.nb_columns))\n",
    "#       unstructured_block = onp.random.normal(self.nb_columns, self.nb_columns)\n",
    "      q, _ = jnp.linalg.qr(unstructured_block)\n",
    "      q = jnp.transpose(q)\n",
    "      block_list.append(q)\n",
    "    remaining_rows = self.nb_rows - nb_full_blocks * self.nb_columns\n",
    "    if remaining_rows > 0:\n",
    "      rng, rng_input = jax.random.split(rng)\n",
    "      unstructured_block = random.normal(rng_input,\n",
    "                                         (self.nb_columns, self.nb_columns))\n",
    "      q, _ = jnp.linalg.qr(unstructured_block)\n",
    "      q = jnp.transpose(q)\n",
    "      block_list.append(q[0:remaining_rows])\n",
    "    final_matrix = jnp.vstack(block_list)\n",
    "\n",
    "    if self.scaling == 0:\n",
    "      multiplier = jnp.linalg.norm(\n",
    "          random.normal(self.key, (self.nb_rows, self.nb_columns)), axis=1)\n",
    "    elif self.scaling == 1:\n",
    "      multiplier = jnp.sqrt(float(self.nb_columns)) * jnp.ones((self.nb_rows))\n",
    "    else:\n",
    "      raise ValueError('Scaling must be one of {0, 1}. Was %s' % self._scaling)\n",
    "\n",
    "    return jnp.matmul(jnp.diag(multiplier), final_matrix)\n",
    "\n",
    "\n",
    "class FastAttention(object):\n",
    "  r\"\"\"Abstract class providing a method for fast attention.\n",
    "  Class is responsible for providing a method <dot_product_attention> for fast\n",
    "  approximate attention.\n",
    "  \"\"\"\n",
    "\n",
    "  __metaclass__ = abc.ABCMeta\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def dot_product_attention(self,\n",
    "                            query,\n",
    "                            key,\n",
    "                            value,\n",
    "                            dtype=jnp.float32,\n",
    "                            bias=None,\n",
    "                            axis=None,\n",
    "                            broadcast_dropout=True,\n",
    "                            dropout_rng=None,\n",
    "                            dropout_rate=0.,\n",
    "                            deterministic=False,\n",
    "                            precision=None):\n",
    "    \"\"\"Computes dot-product attention given query, key, and value.\n",
    "    This is the core function for applying fast approximate dot-product\n",
    "    attention. It calculates the attention weights given query and key and\n",
    "    combines the values using the attention weights. This function supports\n",
    "    multi-dimensional inputs.\n",
    "    Args:\n",
    "      query: queries for calculating attention with shape of [batch_size, dim1,\n",
    "        dim2, ..., dimN, num_heads, mem_channels].\n",
    "      key: keys for calculating attention with shape of [batch_size, dim1, dim2,\n",
    "        ..., dimN, num_heads, mem_channels].\n",
    "      value: values to be used in attention with shape of [batch_size, dim1,\n",
    "        dim2,..., dimN, num_heads, value_channels].\n",
    "      dtype: the dtype of the computation (default: float32)\n",
    "      bias: bias for the attention weights. This can be used for incorporating\n",
    "        autoregressive mask, padding mask, proximity bias.\n",
    "      axis: axises over which the attention is applied.\n",
    "      broadcast_dropout: bool: use a broadcasted dropout along batch dims.\n",
    "      dropout_rng: JAX PRNGKey: to be used for dropout.\n",
    "      dropout_rate: dropout rate.\n",
    "      deterministic: bool, deterministic or not (to apply dropout).\n",
    "      precision: numerical precision of the computation see `jax.lax.Precision`\n",
    "        for details.\n",
    "    Returns:\n",
    "      Output of shape [bs, dim1, dim2, ..., dimN,, num_heads, value_channels].\n",
    "    \"\"\"\n",
    "    raise NotImplementedError('Abstract method')\n",
    "\n",
    "\n",
    "def _numerator_fwd(z_slice_shape, precision, qs, ks, vs):\n",
    "  def body(p, qkv):\n",
    "    (q, k, v) = qkv\n",
    "    tmp= jnp.einsum('...m,...d->...md', k, v, precision=precision)\n",
    "    p = p + tmp\n",
    "    print(\"body p shape: \", p.shape) #(2, 16, 256, 64)\n",
    "    print(\"body tmp shape: \", tmp.shape) #(2, 16, 256, 64)\n",
    "    print(\"body q shape: \", q.shape) #(2, 16, 256)\n",
    "    X_slice = jnp.einsum('...m,...md->...d', q, p, precision=precision)\n",
    "    print(\"body X_slice shape: \", X_slice.shape) #(2, 16, 64)\n",
    "    return p, X_slice\n",
    "  init_value = jnp.zeros(z_slice_shape)\n",
    "  print(\"body qs shape: \", qs.shape)\n",
    "  p, W = lax.scan(body, init_value, (qs, ks, vs))\n",
    "  return W, (p, qs, ks, vs)\n",
    "\n",
    "\n",
    "def _numerator_bwd(z_slice_shape, precision, pqkv, W_ct):\n",
    "  del z_slice_shape\n",
    "  def body(carry, qkv_xct):\n",
    "    p, p_ct = carry\n",
    "    q, k, v, x_ct = qkv_xct\n",
    "    q_ct = jnp.einsum('...d,...md->...m', x_ct, p, precision=precision)\n",
    "    p_ct += jnp.einsum('...d,...m->...md', x_ct, q, precision=precision)\n",
    "    k_ct = jnp.einsum('...md,...d->...m', p_ct, v, precision=precision)\n",
    "    v_ct = jnp.einsum('...md,...m->...d', p_ct, k, precision=precision)\n",
    "    p -= jnp.einsum('...m,...d->...md', k, v, precision=precision)\n",
    "    return (p, p_ct), (q_ct, k_ct, v_ct)\n",
    "  p, qs, ks, vs = pqkv\n",
    "  _, (qs_ct, ks_ct, vs_ct) = lax.scan(\n",
    "      body, (p, jnp.zeros_like(p)), (qs, ks, vs, W_ct), reverse=True)\n",
    "  return qs_ct, ks_ct, vs_ct\n",
    "\n",
    "\n",
    "@functools.partial(jax.custom_vjp, nondiff_argnums=(0, 1))\n",
    "def _numerator(z_slice_shape, precision, qs, ks, vs):\n",
    "  W, _ = _numerator_fwd(z_slice_shape, precision, qs, ks, vs)\n",
    "  return W\n",
    "\n",
    "_numerator.defvjp(_numerator_fwd, _numerator_bwd)\n",
    "\n",
    "# (bs, num_heads, head_dim), (len, bs, num_heads, head_dim)\n",
    "def _denominator_fwd(t_slice_shape, precision, qs, ks):\n",
    "  def body(p, qk):\n",
    "    q, k = qk\n",
    "    p += k\n",
    "    x = jnp.einsum('...m,...m->...', q, p, precision=precision)\n",
    "    print(x.shape)\n",
    "    return p, x\n",
    "\n",
    "  p = jnp.zeros(t_slice_shape)\n",
    "  p, R = lax.scan(body, p, (qs, ks))\n",
    "  return R, (qs, ks, p)\n",
    "\n",
    "\n",
    "def _denominator_bwd(_t_slice_shape, precision, qkp, R_ct):\n",
    "  def body(carry, qkx):\n",
    "    p, p_ct = carry\n",
    "    q, k, x_ct = qkx\n",
    "    q_ct = jnp.einsum('...,...m->...m', x_ct, p, precision=precision)\n",
    "    p_ct += jnp.einsum('...,...m->...m', x_ct, q, precision=precision)\n",
    "    k_ct = p_ct\n",
    "    p -= k\n",
    "    return (p, p_ct), (q_ct, k_ct)\n",
    "  qs, ks, p = qkp\n",
    "  _, (qs_ct, ks_ct) = lax.scan(body, (p, jnp.zeros_like(p)),\n",
    "                               (qs, ks, R_ct), reverse=True)\n",
    "  return (qs_ct, ks_ct)\n",
    "\n",
    "\n",
    "@functools.partial(jax.custom_vjp, nondiff_argnums=(0, 1))\n",
    "def _denominator(t_slice_shape, precision, qs, ks): # (bs, num_heads, head_dim), (len, bs, num_heads, head_dim)\n",
    "  R, _ = _denominator_fwd(t_slice_shape, precision, qs, ks)\n",
    "  return R\n",
    "\n",
    "_denominator.defvjp(_denominator_fwd, _denominator_bwd)\n",
    "\n",
    "\n",
    "class FastAttentionviaLowRankDecomposition(FastAttention):\n",
    "  r\"\"\"Class providing a method for fast attention via low rank decomposition.\n",
    "  Class is responsible for providing a method <dot_product_attention> for fast\n",
    "  dot-product attention with the use of low rank decomposition (e.g. with\n",
    "  random feature maps).\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               matrix_creator,\n",
    "               kernel_feature_creator,\n",
    "               renormalize_attention,\n",
    "               numerical_stabilizer,\n",
    "               redraw_features,\n",
    "               unidirectional,\n",
    "               lax_scan_unroll=1):  # For optimal GPU performance, set to 16.\n",
    "    rng = random.PRNGKey(0)\n",
    "    self.matrix_creator = matrix_creator\n",
    "    self.projection_matrix = self.draw_weights(rng)\n",
    "    self.kernel_feature_creator = kernel_feature_creator\n",
    "    self.renormalize_attention = renormalize_attention\n",
    "    self.numerical_stabilizer = numerical_stabilizer\n",
    "    self.redraw_features = redraw_features\n",
    "    self.unidirectional = unidirectional\n",
    "    self.lax_scan_unroll = lax_scan_unroll\n",
    "\n",
    "  def draw_weights(self, key):\n",
    "    if self.matrix_creator is None:\n",
    "      return None\n",
    "    matrixrng, _ = random.split(key)\n",
    "    projection_matrix = self.matrix_creator(key=matrixrng).get_2d_array()\n",
    "    return projection_matrix\n",
    "\n",
    "  def dot_product_attention(self,\n",
    "                            query,   # (bs, len, num_heads, head_dim)\n",
    "                            key,     # (bs, len, num_heads, head_dim)\n",
    "                            value,   # (bs, len, num_heads, head_dim)\n",
    "                            dtype=jnp.float32,\n",
    "                            bias=None,\n",
    "                            axis=None,\n",
    "                            broadcast_dropout=True,\n",
    "                            dropout_rng=None,\n",
    "                            dropout_rate=0.,\n",
    "                            deterministic=False,\n",
    "                            precision=None):\n",
    "\n",
    "    assert key.shape[:-1] == value.shape[:-1]\n",
    "    assert (query.shape[0:1] == key.shape[0:1] and\n",
    "            query.shape[-1] == key.shape[-1])\n",
    "    if axis is None:\n",
    "      axis = tuple(range(1, key.ndim - 2))  # axis=(1,)\n",
    "    if not isinstance(axis, Iterable):\n",
    "      axis = (axis,)\n",
    "    assert key.ndim == query.ndim\n",
    "    assert key.ndim == value.ndim\n",
    "    for ax in axis:\n",
    "      if not (query.ndim >= 3 and 1 <= ax < query.ndim - 2):\n",
    "        raise ValueError('Attention axis must be between the batch '\n",
    "                         'axis and the last-two axes.')\n",
    "    n = key.ndim\n",
    "    res_value = []\n",
    "    # Constructing projection tensor.\n",
    "    if self.redraw_features:\n",
    "      # TODO(kchoro): Get rid of the constant below.\n",
    "      query_seed = lax.convert_element_type(\n",
    "          jnp.ceil(jnp.sum(query) * 10000000.0), jnp.int32)\n",
    "      rng = random.PRNGKey(query_seed)\n",
    "      self.projection_matrix = self.draw_weights(rng)\n",
    "\n",
    "    # batch_dims is  <bs, <non-attention dims>, num_heads>\n",
    "    batch_dims = tuple(onp.delete(range(n), axis + (n - 1,)))  # (0, 2)\n",
    "    # q & k -> (bs, <non-attention dims>, num_heads, <attention dims>, channels)  #(bs, num_heads, len, head_dim)\n",
    "    qk_perm = batch_dims + axis + (n - 1,)       # (0, 2, 1, 3)\n",
    "    k_extra_perm = axis + batch_dims + (n - 1,)  # (1, 0, 2, 3)\n",
    "    key_extra = key.transpose(k_extra_perm)      # (bs, len, num_heads, head_dim) -> (len, bs, num_heads, head_dim)\n",
    "\n",
    "    key = key.transpose(qk_perm)                 # (bs, len, num_heads, head_dim) -> (bs, num_heads, len, head_dim)\n",
    "    query = query.transpose(qk_perm)             # (bs, len, num_heads, head_dim) -> (bs, num_heads, len, head_dim)\n",
    "    # v -> (bs, <non-attention dims>, num_heads, <attention dims>, channels)\n",
    "    v_perm = batch_dims + axis + (n - 1,)\n",
    "    value = value.transpose(v_perm)\n",
    "    batch_dims_t = tuple(range(len(batch_dims)))  # (0, 1)\n",
    "    attention_dims_t = tuple(                     # (2,)\n",
    "        range(len(batch_dims),\n",
    "              len(batch_dims) + len(axis)))\n",
    "\n",
    "    # Constructing tensors Q^{'} and K^{'}.\n",
    "    query_prime = self.kernel_feature_creator(query, self.projection_matrix,\n",
    "                                              attention_dims_t, batch_dims_t,\n",
    "                                              precision, True)\n",
    "    key_prime = self.kernel_feature_creator(key, self.projection_matrix,\n",
    "                                            attention_dims_t, batch_dims_t,\n",
    "                                            precision, False)\n",
    "#     query_prime = query\n",
    "#     key_prime = key\n",
    "    res_value.append(query_prime)\n",
    "    res_value.append(key_prime)\n",
    "    if self.unidirectional:\n",
    "      index = attention_dims_t[0]  # 2\n",
    "      z_slice_shape = key_prime.shape[0:len(batch_dims_t)] + (\n",
    "          key_prime.shape[-1],) + (value.shape[-1],)  # (bs, num_heads, feat_dim, head_dim)\n",
    "      print(\"z_slice_shape: \", z_slice_shape)\n",
    "      W = _numerator(z_slice_shape, precision,\n",
    "                     jnp.moveaxis(query_prime, index, 0), # (bs, num_heads, len, feat_dim) -> (len, bs, num_heads, feat_dim)\n",
    "                     jnp.moveaxis(key_prime, index, 0), # (bs, num_heads, len, feat_dim) -> (len, bs, num_heads, feat_dim)\n",
    "                     jnp.moveaxis(value, index, 0)) # (bs, num_heads, len, head_dim) -> (len, bs, num_heads, head_dim)\n",
    "      print(\"W shape: \", W.shape)\n",
    "      # Constructing W = (Q^{'}(K^{'})^{T})_{masked}V\n",
    "      W = jnp.moveaxis(W, 0, index)\n",
    "      res_value.append(W)\n",
    "      if not self.renormalize_attention:\n",
    "        # Unidirectional, not-normalized attention.\n",
    "        perm_inv = _invert_perm(qk_perm)\n",
    "        result = W.transpose(perm_inv)\n",
    "        return result, self.projection_matrix, res_value\n",
    "      else:\n",
    "        # Unidirectional, normalized attention.\n",
    "        thick_all_ones = jnp.zeros(key.shape[0:-1]) + jnp.ones(  # (bs, num_heads, len) + (len)\n",
    "            key_extra.shape[0:len(axis)])\n",
    "\n",
    "        index = attention_dims_t[0] # 2\n",
    "        t_slice_shape = key_prime.shape[0:len(batch_dims_t)] + (  # (bs, num_heads, head_dim)\n",
    "            key_prime.shape[-1],)\n",
    "        R = _denominator(t_slice_shape, precision,\n",
    "                         jnp.moveaxis(query_prime, index, 0),\n",
    "                         jnp.moveaxis(key_prime, index, 0))\n",
    "\n",
    "        R = jnp.moveaxis(R, 0, index)\n",
    "    else:\n",
    "      # bidirectional\n",
    "\n",
    "      contract_query = tuple(\n",
    "          range(len(batch_dims) + len(axis),\n",
    "                len(batch_dims) + len(axis) + 1))  # (3,)\n",
    "      contract_z = tuple(range(len(batch_dims), len(batch_dims) + 1))  #(2,)\n",
    "      # Constructing Z = (K^{'})^{T}V\n",
    "      # Z (bs, <non-attention dims>, num_heads, channels_m, channels_v)\n",
    "      value = value.astype(onp.float32)\n",
    "      Z = lax.dot_general(\n",
    "          key_prime,\n",
    "          value,\n",
    "          ((attention_dims_t, attention_dims_t), (batch_dims_t, batch_dims_t)),\n",
    "          precision=precision)\n",
    "      print(Z.shape) # (2, 16, 128, 64)\n",
    "      # Constructing W = Q^{'}Z = Q^{'}(K^{'})^{T}V\n",
    "      # q (bs, <non-attention dims>, num_heads, <attention dims>, channels_m)\n",
    "      # Z (bs, <non-attention dims>, num_heads, channels_m, channels_v)\n",
    "      # W (bs,  <non-attention dims>, num_heads, <attention dims>, channels_v)\n",
    "      W = lax.dot_general(\n",
    "          query_prime,\n",
    "          Z, ((contract_query, contract_z), (batch_dims_t, batch_dims_t)),\n",
    "          precision=precision)\n",
    "      res_value.append(W)\n",
    "      print(W.shape) # (2, 16, 512, 64)\n",
    "      if not self.renormalize_attention:\n",
    "        # Bidirectional, not-normalized attention.\n",
    "        perm_inv = _invert_perm(qk_perm)\n",
    "        result = W.transpose(perm_inv)\n",
    "        return result\n",
    "      else:\n",
    "        # Bidirectional, normalized attention.\n",
    "        \n",
    "        # T_lin = key_prime.sum(axis=2)\n",
    "        # print(onp.sum(onp.abs(T_lin- T)))\n",
    "        thick_all_ones = jnp.zeros(key.shape[0:-1]) + jnp.ones(\n",
    "            key_extra.shape[0:len(axis)])  # (bs, num_heads, len) + (len,)\n",
    "        print(thick_all_ones.shape, jnp.zeros(key.shape[0:-1]).shape, jnp.ones(\n",
    "            key_extra.shape[0:len(axis)]).shape)  # (2, 16, 512) (2, 16, 512) (512,)\n",
    "        contract_key = tuple(\n",
    "            range(len(batch_dims),\n",
    "                  len(batch_dims) + len(axis)))\n",
    "        print(\"contract_key: \", contract_key) # contract_key:  (2,)\n",
    "        contract_thick_all_ones = tuple(\n",
    "            range(thick_all_ones.ndim - len(axis), thick_all_ones.ndim)) \n",
    "        print(\"contract_thick_all_ones: \", contract_thick_all_ones) # (2,)\n",
    "        # Construct T = (K^{'})^{T} 1_L\n",
    "        # k (bs, <non-attention dims>, num_heads, <attention dims>, channels)\n",
    "        T = lax.dot_general(\n",
    "            key_prime,\n",
    "            thick_all_ones, ((contract_key, contract_thick_all_ones),\n",
    "                             (batch_dims_t, batch_dims_t)),     # batch_dims_t: (0, 1)\n",
    "            precision=precision)  # (2, 16, 512, 128), (2, 16, 512)\n",
    "        print(\"T shape: \", T.shape) # (2, 16, 128)\n",
    "        print(\"key_prime shape:\", key_prime.shape)\n",
    "        \n",
    "\n",
    "        # Construct partition function: R = Q^{'} T = Q^{'}(K^{'})^{T} 1_L\n",
    "        # q_p (bs, <non-attention dims>, num_heads, <attention dims>, channs_m)\n",
    "        # T   (bs, <non-attention dims>, num_heads, channels_m)\n",
    "        R = lax.dot_general(\n",
    "            query_prime,\n",
    "            T, (((query_prime.ndim - 1,), (T.ndim - 1,)),\n",
    "                (batch_dims_t, range(0,\n",
    "                                     len(T.shape) - 1))),\n",
    "            precision=precision)\n",
    "        print(\"R shape: \", R.shape) # (2, 16, 512, 128) * (2, 16, 128) -> (2, 16, 512)\n",
    "    \n",
    "#     R_zero_mask = onp.zeros(R.shape, dtype=onp.float32)\n",
    "#     R_numerical_stabilizer_mask = R_zero_mask + 2*self.numerical_stabilizer\n",
    "#     R_add = onp.where(onp.abs(R)<=self.numerical_stabilizer, R_numerical_stabilizer_mask, R_zero_mask)\n",
    "#     R_lin = R+R_add\n",
    "    \n",
    "    R = R + 2 * self.numerical_stabilizer * (\n",
    "        jnp.abs(R) <= self.numerical_stabilizer)\n",
    "#     print(onp.sum(onp.abs(R_lin-R)))\n",
    "    R = jnp.reciprocal(R)\n",
    "    R = jnp.expand_dims(R, len(R.shape))\n",
    "    print(\"R shape: \", R.shape)\n",
    "    # W (bs, <non-attention dims>, num_heads, <attention dims>, channels_v)\n",
    "    # R (bs, <non-attention dims>, num_heads, <attention dims>, extra_channel)\n",
    "    result = W * R\n",
    "    # back to (bs, dim1, dim2, ..., dimN, num_heads, channels)\n",
    "    perm_inv = _invert_perm(qk_perm)\n",
    "    result = result.transpose(perm_inv)\n",
    "    return result, self.projection_matrix, res_value\n",
    "\n",
    "\n",
    "def _invert_perm(perm):\n",
    "  perm_inv = [0] * len(perm)\n",
    "  for i, j in enumerate(perm): # (0, 2, 1, 3) -> (0, 1, 2, 3)\n",
    "    perm_inv[j] = i\n",
    "  return tuple(perm_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1022 17:16:13.275980 140518241646336 xla_bridge.py:131] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "attn_fn = make_fast_softmax_attention(64, nb_features=128, renormalize_attention=True, unidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_slice_shape:  (2, 16, 128, 64)\n",
      "body qs shape:  (512, 2, 16, 128)\n",
      "body p shape:  (2, 16, 128, 64)\n",
      "body tmp shape:  (2, 16, 128, 64)\n",
      "body q shape:  (2, 16, 128)\n",
      "body X_slice shape:  (2, 16, 64)\n",
      "W shape:  (512, 2, 16, 64)\n",
      "(2, 16)\n",
      "R shape:  (2, 16, 512, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([[0.89746666, 0.96294826, 0.32802016, ..., 0.80106604,\n",
       "              0.6299767 , 0.5999515 ],\n",
       "             [0.12154661, 0.33884388, 0.13348654, ..., 0.9184139 ,\n",
       "              0.45374778, 0.4534209 ],\n",
       "             [0.9131297 , 0.8715915 , 0.11985779, ..., 0.8921651 ,\n",
       "              0.25968593, 0.42838234],\n",
       "             ...,\n",
       "             [0.56346226, 0.20953037, 0.20959033, ..., 0.3638113 ,\n",
       "              0.9341765 , 0.8188767 ],\n",
       "             [0.63863283, 0.58186543, 0.22813772, ..., 0.74381965,\n",
       "              0.08203907, 0.7640973 ],\n",
       "             [0.13210867, 0.03546585, 0.26622096, ..., 0.34538937,\n",
       "              0.8901089 , 0.15624902]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1, proj_mat, res_vlaue_orig = attn_fn(query, key, value)\n",
    "res1[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_slice_shape:  (2, 16, 256, 64)\n",
      "body qs shape:  (512, 2, 16, 256)\n",
      "body p shape:  (2, 16, 256, 64)\n",
      "body X_slice shape:  (2, 16, 64)\n",
      "W shape:  (512, 2, 16, 64)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d6d73c661594>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_vlaue_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mres1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "res1, proj_mat, res_vlaue_orig = attn_fn(query, key, value)\n",
    "res1.shape, res1.dtype, proj_mat.shape, proj_mat.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, theta = 1.0, axis = None):\n",
    "    \"\"\"\n",
    "    Compute the softmax of each element along an axis of X.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: ND-Array. Probably should be floats.\n",
    "    theta (optional): float parameter, used as a multiplier\n",
    "        prior to exponentiation. Default = 1.0\n",
    "    axis (optional): axis to compute values along. Default is the\n",
    "        first non-singleton axis.\n",
    "\n",
    "    Returns an array the same size as X. The result will sum to 1\n",
    "    along the specified axis.\n",
    "    \"\"\"\n",
    "\n",
    "    # make X at least 2d\n",
    "    y = onp.atleast_2d(X)\n",
    "\n",
    "    # find axis\n",
    "    if axis is None:\n",
    "        axis = next(j[0] for j in enumerate(y.shape) if j[1] > 1)\n",
    "\n",
    "    # multiply y against the theta parameter,\n",
    "    y = y * float(theta)\n",
    "\n",
    "    # subtract the max for numerical stability\n",
    "    y = y - onp.expand_dims(onp.max(y, axis = axis), axis)\n",
    "\n",
    "    # exponentiate y\n",
    "    y = onp.exp(y)\n",
    "\n",
    "    # take the sum along the specified axis\n",
    "    ax_sum = onp.expand_dims(onp.sum(y, axis = axis), axis)\n",
    "\n",
    "    # finally: divide elementwise\n",
    "    p = y / ax_sum\n",
    "\n",
    "    # flatten if X was 1D\n",
    "    if len(X.shape) == 1: p = p.flatten()\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def attn(q, k, v, bi=True):\n",
    "    # (bs, len, num_heads, head_dim)\n",
    "    size_per_head = q.shape[-1]\n",
    "    q = q.transpose(0, 2, 1, 3) \n",
    "    k = k.transpose(0, 2, 1, 3)\n",
    "    v = v.transpose(0, 2, 1, 3)\n",
    "    attn_score = onp.matmul(q, k.transpose(0, 1, 3, 2))\n",
    "    attn_score = attn_score * (1.0 / math.sqrt(float(size_per_head)))\n",
    "    if not bi:\n",
    "        low_triangular_mask = (onp.tril(onp.ones(attn_score.shape[-2:]), 0)-1)*10000.\n",
    "        attn_score = attn_score + low_triangular_mask[None, None, ...]\n",
    "        print(low_triangular_mask[:5,:5])\n",
    "    attention_probs = softmax(attn_score, axis=-1)\n",
    "    context_layer = onp.matmul(attention_probs, v)\n",
    "    context_layer = context_layer.transpose(0, 2, 1, 3)\n",
    "    return context_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     0. -10000. -10000. -10000. -10000.]\n",
      " [     0.      0. -10000. -10000. -10000.]\n",
      " [     0.      0.      0. -10000. -10000.]\n",
      " [     0.      0.      0.      0. -10000.]\n",
      " [     0.      0.      0.      0.      0.]]\n"
     ]
    }
   ],
   "source": [
    "# res1 = attn_fn(query, key, value)\n",
    "res2 = attn(query, key, value, bi=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 512, 16, 64), (2, 512, 16, 64), dtype('float64'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1.shape, res2.shape, res2.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0071478113532066345"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onp.sum(onp.abs(res1-res2))/onp.prod(res1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0.12154661, 0.33884388, 0.13348654, 0.6301668 , 0.4129922 ,\n",
       "             0.6149349 , 0.06389272, 0.9051133 , 0.40727347, 0.24895161,\n",
       "             0.5860845 , 0.7668272 , 0.10097831, 0.05885285, 0.31033   ,\n",
       "             0.06754939, 0.4960545 , 0.92063195, 0.17771266, 0.31046286,\n",
       "             0.47939324, 0.33753374, 0.7691507 , 0.96271557, 0.9018393 ,\n",
       "             0.93640816, 0.87224925, 0.3215516 , 0.500465  , 0.47145137,\n",
       "             0.8533109 , 0.20703106, 0.7489623 , 0.7105472 , 0.8590447 ,\n",
       "             0.12227874, 0.6935543 , 0.46177316, 0.43412873, 0.6277745 ,\n",
       "             0.6962692 , 0.00691365, 0.9690841 , 0.02563363, 0.27615362,\n",
       "             0.58965224, 0.76309645, 0.33964157, 0.35636517, 0.76667356,\n",
       "             0.17620529, 0.5233161 , 0.89150023, 0.3121818 , 0.6303713 ,\n",
       "             0.20013994, 0.37827945, 0.30610737, 0.5822753 , 0.12941118,\n",
       "             0.9384126 , 0.9184139 , 0.45374778, 0.4534209 ],            dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1[0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12154663, 0.33884388, 0.13348652, 0.63016695, 0.41299236,\n",
       "       0.6149348 , 0.06389271, 0.90511352, 0.40727341, 0.2489516 ,\n",
       "       0.58608454, 0.76682717, 0.10097832, 0.05885284, 0.31033006,\n",
       "       0.06754939, 0.49605447, 0.92063183, 0.17771268, 0.31046286,\n",
       "       0.47939333, 0.33753371, 0.76915061, 0.96271545, 0.90183926,\n",
       "       0.93640804, 0.87224948, 0.32155159, 0.50046504, 0.47145137,\n",
       "       0.8533107 , 0.20703107, 0.74896246, 0.71054715, 0.85904461,\n",
       "       0.12227876, 0.6935544 , 0.46177319, 0.43412879, 0.62777448,\n",
       "       0.69626921, 0.00691366, 0.96908402, 0.02563363, 0.27615362,\n",
       "       0.58965212, 0.76309633, 0.33964157, 0.35636523, 0.76667362,\n",
       "       0.17620531, 0.52331597, 0.89150065, 0.31218186, 0.63037133,\n",
       "       0.20013994, 0.37827951, 0.3061074 , 0.58227539, 0.12941115,\n",
       "       0.93841273, 0.91841376, 0.45374793, 0.45342094])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2[0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1048576\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAGIVJREFUeJzt3X+s3fV93/HnK3ZIsjYJBG4iZDszTdy1JGqdxCWWIk0pZGDIFhMNJqOtuJE3txlsqRJtcdpJtEnQyKaGFY0wkeJhojaG0VZ4iVPPBaKqU/hx0zgQQym3hAUXBE5sSLooZJD3/jgfk8Pl3Hs/vrbvuY6fD+nofr/v7+f7/b7PMfjl749zv6kqJEnq8bJxNyBJOn4YGpKkboaGJKmboSFJ6mZoSJK6GRqSpG6GhiSpm6EhSepmaEiSui0ddwNH22mnnVYrV64cdxuSdFz56le/+u2qmphr3E9caKxcuZLJyclxtyFJx5Uk/6dnnKenJEndDA1JUjdDQ5LUzdCQJHUzNCRJ3QwNSVI3Q0OS1M3QkCR1MzQkSd1+4r4RfiRWbvniC9OPXvXeMXYiSYuTRxqSpG7doZFkSZKvJflCmz8jyd1JHk5yc5KTWv0VbX6qLV85tI2PtfpDSc4bqq9rtakkW4bqI/chSRqPwznS+BDw4ND8p4Crq2oVcBDY1OqbgINV9Wbg6jaOJGcCG4C3AOuAz7QgWgJcC5wPnAlc0sbOtg9J0hh0hUaS5cB7gd9v8wHOBm5tQ7YBF7bp9W2etvycNn49sL2qnq2qbwJTwFntNVVVj1TVD4HtwPo59iFJGoPeI43/Avx74Edt/lTg6ap6rs3vA5a16WXAYwBt+TNt/Av1aevMVJ9tHy+SZHOSySST+/fv73xLkqTDNWdoJPnHwFNV9dXh8oihNceyo1V/abHq+qpaU1VrJibmfIaIJGmeem65fRfwviQXAK8EXsPgyOPkJEvbkcBy4PE2fh+wAtiXZCnwWuDAUP2Q4XVG1b89yz4kSWMw55FGVX2sqpZX1UoGF7LvqKp/DtwJXNSGbQRua9M72jxt+R1VVa2+od1ddQawCrgHuBdY1e6UOqntY0dbZ6Z9SJLG4Ei+p/FR4MNJphhcf7ih1W8ATm31DwNbAKpqL3AL8ADwp8BlVfV8O4q4HNjF4O6sW9rY2fYhSRqDw/pGeFV9Gfhym36EwZ1P08f8ALh4hvWvBK4cUd8J7BxRH7kPSdJ4+I1wSVI3Q0OS1M3QkCR1MzQkSd0MDUlSN0NDktTN0JAkdTM0JEndDA1JUjdDQ5LUzdCQJHUzNCRJ3QwNSVI3Q0OS1M3QkCR1MzQkSd3mDI0kr0xyT5KvJ9mb5Hda/cYk30yyp71Wt3qSXJNkKsl9Sd4+tK2NSR5ur41D9Xckub+tc02StPrrkuxu43cnOeXofwSSpF49RxrPAmdX1S8Cq4F1Sda2Zf+uqla3155WO5/B879XAZuB62AQAMAVwDsZPI3viqEQuK6NPbTeulbfAtxeVauA29u8JGlM5gyNGvi7Nvvy9qpZVlkP3NTWuws4OcnpwHnA7qo6UFUHgd0MAuh04DVV9ZWqKuAm4MKhbW1r09uG6pKkMei6ppFkSZI9wFMM/uK/uy26sp2CujrJK1ptGfDY0Or7Wm22+r4RdYA3VNUTAO3n62fob3OSySST+/fv73lLkqR56AqNqnq+qlYDy4GzkrwV+Bjwc8AvAa8DPtqGZ9Qm5lHvVlXXV9WaqlozMTFxOKtKkg7DYd09VVVPA18G1lXVE+0U1LPAf2dwnQIGRworhlZbDjw+R335iDrAk+30Fe3nU4fTryTp6Oq5e2oiyclt+lXAe4C/GvrLPAyuNXyjrbIDuLTdRbUWeKadWtoFnJvklHYB/FxgV1v2vSRr27YuBW4b2tahu6w2DtUlSWOwtGPM6cC2JEsYhMwtVfWFJHckmWBwemkP8Ott/E7gAmAK+D7wAYCqOpDkE8C9bdzHq+pAm/4gcCPwKuBL7QVwFXBLkk3At4CL5/tGJUlHbs7QqKr7gLeNqJ89w/gCLpth2VZg64j6JPDWEfXvAOfM1aMkaWH4jXBJUjdDQ5LUzdCQJHUzNCRJ3QwNSVI3Q0OS1M3QkCR1MzQkSd0MDUlSN0NDktTN0JAkdTM0JEndDA1JUjdDQ5LUzdCQJHUzNCRJ3Xoe9/rKJPck+XqSvUl+p9XPSHJ3koeT3JzkpFZ/RZufastXDm3rY63+UJLzhurrWm0qyZah+sh9SJLGo+dI41ng7Kr6RWA1sK49+/tTwNVVtQo4CGxq4zcBB6vqzcDVbRxJzgQ2AG8B1gGfSbKkPUb2WuB84EzgkjaWWfYhSRqDOUOjBv6uzb68vQo4G7i11bcBF7bp9W2etvycJGn17VX1bFV9k8EzxM9qr6mqeqSqfghsB9a3dWbahyRpDLquabQjgj3AU8Bu4G+Ap6vquTZkH7CsTS8DHgNoy58BTh2uT1tnpvqps+xDkjQGXaFRVc9X1WpgOYMjg58fNaz9zAzLjlb9JZJsTjKZZHL//v2jhkiSjoLDunuqqp4GvgysBU5OsrQtWg483qb3ASsA2vLXAgeG69PWman+7Vn2Mb2v66tqTVWtmZiYOJy3JEk6DD13T00kOblNvwp4D/AgcCdwURu2EbitTe9o87Tld1RVtfqGdnfVGcAq4B7gXmBVu1PqJAYXy3e0dWbahyRpDJbOPYTTgW3tLqeXAbdU1ReSPABsT/JJ4GvADW38DcDnkkwxOMLYAFBVe5PcAjwAPAdcVlXPAyS5HNgFLAG2VtXetq2PzrAPSdIYzBkaVXUf8LYR9UcYXN+YXv8BcPEM27oSuHJEfSews3cfkqTx8BvhkqRuPaenTkgrt3zxhelHr3rvGDuRpMXDIw1JUjdDQ5LUzdCQJHUzNCRJ3QwNSVI3Q0OS1M3QkCR1MzQkSd0MDUlSN0NDktTN0JAkdTM0JEndDA1JUjdDQ5LUredxryuS3JnkwSR7k3yo1X87yd8m2dNeFwyt87EkU0keSnLeUH1dq00l2TJUPyPJ3UkeTnJze+wr7dGwN7fxdydZeTTfvCTp8PQcaTwHfKSqfh5YC1yW5My27OqqWt1eOwHasg3AW4B1wGeSLGmPi70WOB84E7hkaDufattaBRwENrX6JuBgVb0ZuLqNkySNyZyhUVVPVNVftunvAQ8Cy2ZZZT2wvaqerapvAlMMHtl6FjBVVY9U1Q+B7cD6JAHOBm5t628DLhza1rY2fStwThsvSRqDw7qm0U4PvQ24u5UuT3Jfkq1JTmm1ZcBjQ6vta7WZ6qcCT1fVc9PqL9pWW/5MGy9JGoPu0Ejy08AfAb9RVd8FrgPeBKwGngB+99DQEavXPOqzbWt6b5uTTCaZ3L9//6zvQ5I0f12hkeTlDALjD6rqjwGq6smqer6qfgR8lsHpJxgcKawYWn058Pgs9W8DJydZOq3+om215a8FDkzvr6qur6o1VbVmYmKi5y1Jkuah5+6pADcAD1bVp4fqpw8Nez/wjTa9A9jQ7nw6A1gF3APcC6xqd0qdxOBi+Y6qKuBO4KK2/kbgtqFtbWzTFwF3tPGSpDFYOvcQ3gX8CnB/kj2t9psM7n5azeB00aPArwFU1d4ktwAPMLjz6rKqeh4gyeXALmAJsLWq9rbtfRTYnuSTwNcYhBTt5+eSTDE4wthwBO9VknSE5gyNqvoLRl9b2DnLOlcCV46o7xy1XlU9wo9Pbw3XfwBcPFePkqSF4TfCJUndDA1JUjdDQ5LUzdCQJHUzNCRJ3QwNSVI3Q0OS1M3QkCR1MzQkSd0MDUlSN0NDktTN0JAkdTM0JEndDA1JUjdDQ5LUzdCQJHXredzriiR3Jnkwyd4kH2r11yXZneTh9vOUVk+Sa5JMJbkvyduHtrWxjX84ycah+juS3N/WuaY9YnbGfUiSxqPnSOM54CNV9fPAWuCyJGcCW4Dbq2oVcHubBzifwXPBVwGbgetgEADAFcA7GTyl74qhELiujT203rpWn2kfkqQxmDM0quqJqvrLNv094EFgGbAe2NaGbQMubNPrgZtq4C7g5CSnA+cBu6vqQFUdBHYD69qy11TVV6qqgJumbWvUPiRJY3BY1zSSrATeBtwNvKGqnoBBsACvb8OWAY8Nrbav1War7xtRZ5Z9SJLGoDs0kvw08EfAb1TVd2cbOqJW86h3S7I5yWSSyf379x/OqpKkw9AVGkleziAw/qCq/riVn2ynlmg/n2r1fcCKodWXA4/PUV8+oj7bPl6kqq6vqjVVtWZiYqLnLUmS5qHn7qkANwAPVtWnhxbtAA7dAbURuG2ofmm7i2ot8Ew7tbQLODfJKe0C+LnArrbse0nWtn1dOm1bo/YhSRqDpR1j3gX8CnB/kj2t9pvAVcAtSTYB3wIubst2AhcAU8D3gQ8AVNWBJJ8A7m3jPl5VB9r0B4EbgVcBX2ovZtmHJGkM5gyNqvoLRl93ADhnxPgCLpthW1uBrSPqk8BbR9S/M2ofkqTx8BvhkqRuhoYkqZuhIUnqZmhIkroZGpKkboaGJKmboSFJ6mZoSJK6GRqSpG6GhiSpm6EhSepmaEiSuhkakqRuhoYkqZuhIUnqZmhIkrr1PLnvhLdyyxdfmH70qveOsRNJGq+eZ4RvTfJUkm8M1X47yd8m2dNeFwwt+1iSqSQPJTlvqL6u1aaSbBmqn5Hk7iQPJ7k5yUmt/oo2P9WWrzxab1qSND89p6duBNaNqF9dVavbaydAkjOBDcBb2jqfSbIkyRLgWuB84EzgkjYW4FNtW6uAg8CmVt8EHKyqNwNXt3GSpDGaMzSq6s+BA53bWw9sr6pnq+qbwBRwVntNVdUjVfVDYDuwPkmAs4Fb2/rbgAuHtrWtTd8KnNPGS5LG5EguhF+e5L52+uqUVlsGPDY0Zl+rzVQ/FXi6qp6bVn/RttryZ9r4l0iyOclkksn9+/cfwVuSJM1mvqFxHfAmYDXwBPC7rT7qSKDmUZ9tWy8tVl1fVWuqas3ExMRsfUuSjsC8QqOqnqyq56vqR8BnGZx+gsGRwoqhocuBx2epfxs4OcnSafUXbastfy39p8kkScfAvEIjyelDs+8HDt1ZtQPY0O58OgNYBdwD3AusandKncTgYvmOqirgTuCitv5G4LahbW1s0xcBd7TxkqQxmfN7Gkk+D7wbOC3JPuAK4N1JVjM4XfQo8GsAVbU3yS3AA8BzwGVV9XzbzuXALmAJsLWq9rZdfBTYnuSTwNeAG1r9BuBzSaYYHGFsOOJ3K0k6InOGRlVdMqJ8w4jaofFXAleOqO8Edo6oP8KPT28N138AXDxXf5KkheOvEZEkdTM0JEndDA1JUjdDQ5LUzdCQJHUzNCRJ3QwNSVI3Q0OS1M3QkCR1MzQkSd0MDUlSN0NDktTN0JAkdTM0JEndDA1JUjdDQ5LUbc7QSLI1yVNJvjFUe12S3Ukebj9PafUkuSbJVJL7krx9aJ2NbfzDSTYO1d+R5P62zjVJMts+JEnj03OkcSOwblptC3B7Va0Cbm/zAOczeC74KmAzcB0MAoDBY2LfyeApfVcMhcB1beyh9dbNsQ9J0pjMGRpV9ecMntE9bD2wrU1vAy4cqt9UA3cBJyc5HTgP2F1VB6rqILAbWNeWvaaqvlJVBdw0bVuj9iFJGpP5XtN4Q1U9AdB+vr7VlwGPDY3b12qz1feNqM+2j5dIsjnJZJLJ/fv3z/MtSZLmcrQvhGdEreZRPyxVdX1VramqNRMTE4e7uiSp03xD48l2aon286lW3wesGBq3HHh8jvryEfXZ9iFJGpP5hsYO4NAdUBuB24bql7a7qNYCz7RTS7uAc5Oc0i6Anwvsasu+l2Rtu2vq0mnbGrUPSdKYLJ1rQJLPA+8GTkuyj8FdUFcBtyTZBHwLuLgN3wlcAEwB3wc+AFBVB5J8Ari3jft4VR26uP5BBndovQr4Unsxyz4kSWMyZ2hU1SUzLDpnxNgCLpthO1uBrSPqk8BbR9S/M2ofkqTx8RvhkqRuhoYkqZuhIUnqZmhIkroZGpKkbnPePaUXW7nliy9MP3rVe8fYiSQtPI80JEndDA1JUjdDQ5LUzdCQJHUzNCRJ3QwNSVI3Q0OS1M3QkCR1MzQkSd0MDUlStyMKjSSPJrk/yZ4kk632uiS7kzzcfp7S6klyTZKpJPclefvQdja28Q8n2ThUf0fb/lRbN0fSryTpyByNI41frqrVVbWmzW8Bbq+qVcDtbR7gfGBVe20GroNByDB4hOw7gbOAKw4FTRuzeWi9dUehX0nSPB2L01PrgW1tehtw4VD9phq4Czg5yenAecDuqjpQVQeB3cC6tuw1VfWV9hjZm4a2JUkagyMNjQL+V5KvJtncam+oqicA2s/Xt/oy4LGhdfe12mz1fSPqkqQxOdJfjf6uqno8yeuB3Un+apaxo65H1DzqL93wILA2A7zxjW+cvWNJ0rwd0ZFGVT3efj4F/AmDaxJPtlNLtJ9PteH7gBVDqy8HHp+jvnxEfVQf11fVmqpaMzExcSRvSZI0i3mHRpKfSvLqQ9PAucA3gB3AoTugNgK3tekdwKXtLqq1wDPt9NUu4Nwkp7QL4OcCu9qy7yVZ2+6aunRoW5KkMTiS01NvAP6k3QW7FPjDqvrTJPcCtyTZBHwLuLiN3wlcAEwB3wc+AFBVB5J8Ari3jft4VR1o0x8EbgReBXypvSRJY5LBjUk/OdasWVOTk5PzWnf4Ua6Hy0e/SjqeJfnq0FcnZuQ3wiVJ3QwNSVI3Q0OS1M3QkCR1MzQkSd0MDUlSN0NDktTN0JAkdTvSX1ioZviLgX7RT9JPKo80JEndDA1JUjdDQ5LUzdCQJHXzQvgx4EVxST+pPNKQJHXzSOMYm/6MDo88JB3PFv2RRpJ1SR5KMpVky7j7kaQT2aI+0kiyBLgW+EfAPuDeJDuq6oHxdjZ/Xu+QdDxb1KEBnAVMVdUjAEm2A+uB4zY0hs30eFnDRNJitdhDYxnw2ND8PuCdY+plwRzJs8qHGT6SjrbFHhoZUauXDEo2A5vb7N8leWie+zsN+PY8111IXX3mUwvQyeyOh8/zeOgR7PNoOh56hIXv8+/3DFrsobEPWDE0vxx4fPqgqroeuP5Id5ZksqrWHOl2jjX7PHqOhx7BPo+m46FHWLx9Lva7p+4FViU5I8lJwAZgx5h7kqQT1qI+0qiq55JcDuwClgBbq2rvmNuSpBPWog4NgKraCexcoN0d8SmuBWKfR8/x0CPY59F0PPQIi7TPVL3kurIkSSMt9msakqRF5IQMjbl+NUmSVyS5uS2/O8nKhe+yq89/mOQvkzyX5KJF2uOHkzyQ5L4ktyfpuq1vDH3+epL7k+xJ8hdJzlyMfQ6NuyhJJVnwu2s6PstfTbK/fZZ7kvzLhe6xp8825p+1/z73JvnDhe6x9TDX53n10Gf510meHkefL6iqE+rF4IL63wA/A5wEfB04c9qYfw38tza9Abh5kfa5EvgF4CbgokXa4y8Df69Nf3ARf5avGZp+H/Cni7HPNu7VwJ8DdwFrFluPwK8C/3WhP7959LkK+BpwSpt//WLsc9r4f8PghqCxfbYn4pHGC7+apKp+CBz61STD1gPb2vStwDlJRn3R8Fias8+qerSq7gN+tMC9HdLT451V9f02exeD79ostJ4+vzs0+1OM+BLpAuj5bxPgE8B/An6wkM01vT2OW0+f/wq4tqoOAlTVUwvcIxz+53kJ8PkF6WwGJ2JojPrVJMtmGlNVzwHPAKcuSHcjemhG9Tluh9vjJuBLx7Sj0br6THJZkr9h8Bfyv12g3obN2WeStwErquoLC9nYkN4/83/aTknemmTFiOXHWk+fPwv8bJL/neSuJOsWrLsf6/5/qJ3aPQO4YwH6mtGJGBo9v5qk69eXHGOLoYe5dPeY5F8Aa4D/fEw7Gq2rz6q6tqreBHwU+A/HvKuXmrXPJC8DrgY+smAdvVTPZ/k/gZVV9QvAn/Hjo/aF1NPnUganqN7N4F/wv5/k5GPc13SH8//5BuDWqnr+GPYzpxMxNHp+NckLY5IsBV4LHFiQ7kb00Iz8FSpj1tVjkvcAvwW8r6qeXaDehh3uZ7kduPCYdjTaXH2+Gngr8OUkjwJrgR0LfDF8zs+yqr4z9Of8WeAdC9TbsN7/z2+rqv9XVd8EHmIQIgvpcP7b3MCYT00BJ+SF8KXAIwwO8w5deHrLtDGX8eIL4bcsxj6Hxt7IeC6E93yWb2NwoW/VIv8zXzU0/U+AycXY57TxX2bhL4T3fJanD02/H7hrMX6WwDpgW5s+jcFpolMXW59t3D8AHqV9t26cr7HufGxvGi4A/rr9ZfZbrfZxBv8SBngl8D+AKeAe4GcWaZ+/xOBfKv8X+A6wdxH2+GfAk8Ce9tqxSD/L3wP2th7vnO0v63H2OW3sgodG52f5H9tn+fX2Wf7cYvwsGZwa+jSD5/PcD2xYjH22+d8GrhpHf9NffiNcktTtRLymIUmaJ0NDktTN0JAkdTM0JEndDA1JUjdDQ5LUzdCQJHUzNCRJ3f4/BCmhH9sX5esAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diff_ratio = onp.abs(res1-res2)/res2\n",
    "import matplotlib.pyplot as plt\n",
    "count = onp.prod(res1.shape)\n",
    "print(count)\n",
    "plt.hist(diff_ratio.reshape(-1), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,) (2,1024,16,64) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e51537466b00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mres2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,) (2,1024,16,64) "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "diff = onp.abs(res1 - res2)\n",
    "count = onp.prod(res1.shape)\n",
    "print(count)\n",
    "plt.hist(diff.reshape(-1), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 16, 32, 64)\n",
      "(2, 16, 1024, 64)\n",
      "(2, 16, 1024) (2, 16, 1024) (1024,)\n",
      "contract_key:  (2,)\n",
      "contract_thick_all_ones:  (2,)\n",
      "T shape:  (2, 16, 32)\n",
      "key_prime shape: (2, 16, 1024, 32)\n",
      "R shape:  (2, 16, 1024)\n",
      "R shape:  (2, 16, 1024, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,) (2,1024,16,64) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c29b64affb87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mres1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mres2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmean_abs_diff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mres2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0monp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"feat_dim: %d, mean abs diff %.6f\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_abs_diff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,) (2,1024,16,64) "
     ]
    }
   ],
   "source": [
    "for feat_dim in [32, 64, 128, 256, 512, 1024, 2048, 4096]:\n",
    "    shape = (2, 1024, 16, 64)\n",
    "    onp.random.seed(feat_dim+10)\n",
    "    query = onp.random.rand(*shape) #(bs, len, num_heads, head_dim)\n",
    "    key = onp.random.rand(*shape)\n",
    "    value = onp.random.rand(*shape)\n",
    "    attn_fn = make_fast_softmax_attention(64, nb_features=feat_dim, renormalize_attention=True)\n",
    "    res1 = attn_fn(query, key, value)\n",
    "    res2 = attn(query, key, value)\n",
    "    mean_abs_diff = onp.sum(onp.abs(res1-res2))/onp.prod(res1.shape)\n",
    "    print(\"feat_dim: %d, mean abs diff %.6f\"%(feat_dim, mean_abs_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_dim: 32, mean abs diff 0.002560\n",
      "feat_dim: 64, mean abs diff 0.002847\n",
      "feat_dim: 128, mean abs diff 0.001985\n",
      "feat_dim: 256, mean abs diff 0.001317\n",
      "feat_dim: 512, mean abs diff 0.001854\n",
      "feat_dim: 1024, mean abs diff 0.001208\n",
      "feat_dim: 2048, mean abs diff 0.000938\n",
      "feat_dim: 4096, mean abs diff 0.001445\n"
     ]
    }
   ],
   "source": [
    "for feat_dim in [32, 64, 128, 256, 512, 1024, 2048, 4096]:\n",
    "    shape = (2, 2048, 16, 64)\n",
    "    onp.random.seed(feat_dim+10)\n",
    "    query = onp.random.rand(*shape) #(bs, len, num_heads, head_dim)\n",
    "    key = onp.random.rand(*shape)\n",
    "    value = onp.random.rand(*shape)\n",
    "    attn_fn = make_fast_softmax_attention(64, nb_features=feat_dim, renormalize_attention=True)\n",
    "    res1 = attn_fn(query, key, value)\n",
    "    res2 = attn(query, key, value)\n",
    "    mean_abs_diff = onp.sum(onp.abs(res1-res2))/onp.prod(res1.shape)\n",
    "    print(\"feat_dim: %d, mean abs diff %.6f\"%(feat_dim, mean_abs_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atten2(q, k, v):\n",
    "    # (bs, len, num_heads, head_dim)\n",
    "    size_per_head = q.shape[-1]\n",
    "    n = q.shape[-2]\n",
    "    q = q.transpose(0, 2, 1, 3) \n",
    "    k = k.transpose(0, 2, 1, 3)\n",
    "    v = v.transpose(0, 2, 1, 3) # (bs, num_heads, len, head_dim)\n",
    "    q_softmax = softmax(q, axis=-1) * 1.0/ math.sqrt(n)\n",
    "    k_softmax = softmax(k, axis=-2) * 1.0/ math.sqrt(n)\n",
    "    \n",
    "    kv = onp.matmul(k_softmax.transpose(0, 1, 3, 2), v) # (bs, num_heads, head_dim, head_dim)\n",
    "    context_layer = onp.matmul(q_softmax, kv) # (bs, num_heads, len, head_dim)\n",
    "    context_layer = context_layer.transpose(0, 2, 1, 3)\n",
    "    return context_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.468458336257408"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# res1 = attn_fn(query, key, value)\n",
    "res2 = attn(query, key, value)\n",
    "res3 = atten2(query, key, value)\n",
    "onp.sum(onp.abs(res3-res2))/onp.prod(res1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49428869, 0.50142085, 0.49378735, 0.51016673, 0.50480644,\n",
       "       0.50228521, 0.49877265, 0.48711779, 0.49409655, 0.49744293,\n",
       "       0.48953286, 0.49621849, 0.49413408, 0.48669814, 0.49689139,\n",
       "       0.50418089, 0.50991587, 0.49752703, 0.5046443 , 0.5069506 ,\n",
       "       0.49321097, 0.51191124, 0.50095813, 0.49457211, 0.49548192,\n",
       "       0.50744094, 0.49850628, 0.5037728 , 0.50300514, 0.50454333,\n",
       "       0.50665546, 0.49696828, 0.49577153, 0.50219054, 0.49282323,\n",
       "       0.49895454, 0.52016535, 0.50153318, 0.49412961, 0.49616413,\n",
       "       0.49997897, 0.49429182, 0.48865578, 0.49620458, 0.49594426,\n",
       "       0.50307947, 0.49252937, 0.4967054 , 0.49785785, 0.51761538,\n",
       "       0.51158979, 0.51353982, 0.5145384 , 0.50457855, 0.49715276,\n",
       "       0.4977709 , 0.48818036, 0.50533134, 0.49335693, 0.49985794,\n",
       "       0.50688144, 0.48960757, 0.50037194, 0.48997852])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2[0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03091257, 0.03140684, 0.03083307, 0.03179559, 0.03149994,\n",
       "       0.03141004, 0.0312512 , 0.03047129, 0.03089646, 0.03111978,\n",
       "       0.03063871, 0.03101448, 0.03088606, 0.03034551, 0.03107876,\n",
       "       0.03158305, 0.03187562, 0.03115086, 0.03154974, 0.03162817,\n",
       "       0.03083868, 0.03199402, 0.03132343, 0.03095931, 0.03105424,\n",
       "       0.03172261, 0.03126309, 0.03154043, 0.0314578 , 0.03154563,\n",
       "       0.0317354 , 0.03117126, 0.03100561, 0.0313605 , 0.03074946,\n",
       "       0.03115346, 0.03244557, 0.03139081, 0.03089449, 0.03102182,\n",
       "       0.03122772, 0.03093461, 0.03049361, 0.03107778, 0.03104208,\n",
       "       0.03147625, 0.03077831, 0.03106927, 0.03119094, 0.0322043 ,\n",
       "       0.03200334, 0.03206791, 0.03214676, 0.03149196, 0.03098876,\n",
       "       0.03103158, 0.03046521, 0.03157382, 0.03082395, 0.03120166,\n",
       "       0.03171428, 0.03068032, 0.03127315, 0.0305605 ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res3[0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = onp.ones((2,3,4,5))\n",
    "b = onp.ones((2,3,5))\n",
    "c = onp.einsum('...nd,...d->...n', a, b)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "\n",
    "def get_shape_list(tensor, expected_rank=None, name=None):\n",
    "    \"\"\"Returns a list of the shape of tensor, preferring static dimensions.\n",
    "    Args:\n",
    "      tensor: A tf.Tensor object to find the shape of.\n",
    "      expected_rank: (optional) int. The expected rank of `tensor`. If this is\n",
    "        specified and the `tensor` has a different rank, and exception will be\n",
    "        thrown.\n",
    "      name: Optional name of the tensor for the error message.\n",
    "    Returns:\n",
    "      A list of dimensions of the shape of tensor. All static dimensions will\n",
    "      be returned as python integers, and dynamic dimensions will be returned\n",
    "      as tf.Tensor scalars.\n",
    "    \"\"\"\n",
    "    if name is None:\n",
    "        name = tensor.name\n",
    "\n",
    "    if expected_rank is not None:\n",
    "        assert_rank(tensor, expected_rank, name)\n",
    "\n",
    "    shape = tensor.shape.as_list()\n",
    "\n",
    "    non_static_indexes = []\n",
    "    for (index, dim) in enumerate(shape):\n",
    "        if dim is None:\n",
    "            non_static_indexes.append(index)\n",
    "\n",
    "    if not non_static_indexes:\n",
    "        return shape\n",
    "\n",
    "    dyn_shape = tf.shape(tensor)\n",
    "    for index in non_static_indexes:\n",
    "        shape[index] = dyn_shape[index]\n",
    "    return shape\n",
    "\n",
    "def shape_list(x):\n",
    "    \"\"\"\n",
    "    Deal with dynamic shape in tensorflow cleanly.\n",
    "\n",
    "    Args:\n",
    "        x (:obj:`tf.Tensor`): The tensor we want the shape of.\n",
    "\n",
    "    Returns:\n",
    "        :obj:`List[int]`: The shape of the tensor as a list.\n",
    "    \"\"\"\n",
    "    static = x.shape.as_list()\n",
    "    dynamic = tf.shape(x)\n",
    "    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n",
    "\n",
    "\n",
    "def nonnegative_softmax_kernel_feature_creator(data,\n",
    "                                               projection_matrix,\n",
    "                                               is_query,\n",
    "                                               normalize_data=True,\n",
    "                                               eps=0.000001):\n",
    "    \"\"\"Constructs nonnegative kernel features for fast softmax attention.\n",
    "    Args:\n",
    "    data: input for which features are computes\n",
    "    projection_matrix: random matrix used to compute features\n",
    "    attention_dims_t: tuple of attention dimensions\n",
    "    batch_dims_t: tuple of batch dimensions\n",
    "    precision: precision parameter\n",
    "    is_query: predicate indicating whether input data corresponds to queries or\n",
    "      keys\n",
    "    normalize_data: predicate indicating whether data should be normalized,\n",
    "    eps: numerical stabilizer.\n",
    "    Returns:\n",
    "    Random features for fast softmax attention.\n",
    "    \"\"\"\n",
    "    if data.dtype != projection_matrix.dtype:\n",
    "        projection_matrix = tf.saturate_cast(projection_matrix, data.dtype)\n",
    "\n",
    "    if normalize_data:\n",
    "        # We have e^{qk^T/sqrt{d}} = e^{q_norm k_norm^T}, where\n",
    "        # w_norm = w * data_normalizer for w in {q,k}.\n",
    "        data_shape = get_shape_list(data)\n",
    "        data_normalizer = 1.0 / (math.sqrt(math.sqrt(float(data_shape[-1]))))\n",
    "    else:\n",
    "        data_normalizer = 1.0\n",
    "    ratio = 1.0 / math.sqrt(float(get_shape_list(projection_matrix)[0]))\n",
    "    # data_mod_shape = data.shape[:len(data.shape)-2] + projection_matrix.shape\n",
    "    data_mod_shape = get_shape_list(data)[:len(data.shape)-2] + get_shape_list(projection_matrix)\n",
    "    data_thick_random_matrix = tf.zeros(data_mod_shape, dtype=data.dtype) + projection_matrix  # broadcast to batch axis\n",
    "\n",
    "    data_dash = tf.einsum('...id,...jd->...ij', (data_normalizer*data), data_thick_random_matrix)\n",
    "\n",
    "    diag_data = data**2\n",
    "    diag_data = tf.reduce_sum(diag_data, axis=-1)\n",
    "    diag_data = (diag_data / 2.0) * data_normalizer**2\n",
    "    diag_data = tf.expand_dims(diag_data, axis=-1)\n",
    "    \n",
    "    if is_query:\n",
    "        data_dash = ratio * (\n",
    "            tf.exp(data_dash - diag_data - tf.reduce_max(data_dash, axis=-1, keep_dims=True)) + eps)\n",
    "    else:\n",
    "        data_dash = ratio * (\n",
    "            tf.exp(data_dash - diag_data - tf.reduce_max(data_dash)) + eps)\n",
    "    \n",
    "    return data_dash\n",
    "\n",
    "@tf.custom_gradient\n",
    "def my_eig(x):\n",
    "    e, v = np.linalg.qr(x)\n",
    "    def grad(grad_e, grad_v):\n",
    "        return None\n",
    "    return (e, v), grad\n",
    "\n",
    "@tf.custom_gradient\n",
    "def qr_wo_grad(x):\n",
    "    q, r = tf.qr(x, full_matrices=False)\n",
    "    q, r = tf.stop_gradient(q), tf.stop_gradient(r)\n",
    "    def grad(dq, dr):\n",
    "        return dq\n",
    "    return (q, r), grad\n",
    "\n",
    "def orthogonal_matrix_chunk(cols, dtype):\n",
    "    use_numpy = False\n",
    "    if use_numpy:\n",
    "        unstructured_block = tf.random_normal((cols, cols), dtype=tf.float32)\n",
    "        # with tf.GradientTape() as tape:\n",
    "        #     tape.watch(unstructured_block)\n",
    "        q, _ = tf.py_function(func=my_eig, inp=[unstructured_block], Tout=[tf.float32, tf.float32])\n",
    "        q.set_shape(unstructured_block.get_shape())\n",
    "        q = tf.saturate_cast(q, dtype=dtype)\n",
    "        # print(q.shape)\n",
    "    else:\n",
    "        # unstructured_block = tf.stop_gradient(tf.random_normal((cols, cols), dtype=dtype))\n",
    "        # q, r = tf.qr(unstructured_block, full_matrices=False)\n",
    "        # q, r = tf.stop_gradient(q), tf.stop_gradient(r)\n",
    "        # q, r = qr_wo_grad(unstructured_block)\n",
    "        unstructured_block = tf.random_normal((cols, cols), dtype=tf.float32)\n",
    "        q, r = tf.qr(unstructured_block, full_matrices=False)\n",
    "    return tf.transpose(q)\n",
    "\n",
    "\n",
    "def gaussian_orthogonal_random_matrix(nb_rows, nb_columns, scaling = 0, dtype=tf.float16):\n",
    "    nb_full_blocks = int(nb_rows / nb_columns)\n",
    "\n",
    "    block_list = []\n",
    "\n",
    "    for _ in range(nb_full_blocks):\n",
    "        q = orthogonal_matrix_chunk(nb_columns, dtype=dtype)\n",
    "        block_list.append(q)\n",
    "\n",
    "    remaining_rows = nb_rows - nb_full_blocks * nb_columns\n",
    "    if remaining_rows > 0:\n",
    "        q = orthogonal_matrix_chunk(nb_columns, dtype=dtype)\n",
    "        block_list.append(q[:remaining_rows])\n",
    "\n",
    "    final_matrix = tf.saturate_cast(tf.concat(block_list, 0), dtype=dtype)\n",
    "\n",
    "    if scaling == 0:\n",
    "        multiplier = tf.norm(tf.random_normal((nb_rows, nb_columns), dtype=dtype), axis=1)\n",
    "    elif scaling == 1:\n",
    "        multiplier = math.sqrt((float(nb_columns))) * tf.ones((nb_rows,), dtype=dtype)\n",
    "    else:\n",
    "        raise ValueError(f'Invalid scaling {scaling}')\n",
    "\n",
    "    return tf.matmul(tf.diag(multiplier), final_matrix)\n",
    "\n",
    "\n",
    "def np_orthogonal_matrix_chunk(cols):\n",
    "    unstructured_block = np.random.normal(size=(cols, cols))\n",
    "    q, _ = np.linalg.qr(unstructured_block)\n",
    "    return q.T\n",
    "\n",
    "\n",
    "def np_gaussian_orthogonal_random_matrix(nb_rows, nb_columns, scaling = 0, dtype=tf.float16):\n",
    "    nb_full_blocks = int(nb_rows / nb_columns)\n",
    "\n",
    "    block_list = []\n",
    "\n",
    "    for _ in range(nb_full_blocks):\n",
    "        q = np_orthogonal_matrix_chunk(nb_columns)\n",
    "        block_list.append(q)\n",
    "\n",
    "    remaining_rows = nb_rows - nb_full_blocks * nb_columns\n",
    "    if remaining_rows > 0:\n",
    "        q = np_orthogonal_matrix_chunk(nb_columns)\n",
    "        block_list.append(q[:remaining_rows])\n",
    "\n",
    "    final_matrix = np.concatenate(block_list, axis=0)\n",
    "    final_matrix = tf.convert_to_tensor(final_matrix, dtype=dtype)\n",
    "    if scaling == 0:\n",
    "        multiplier = tf.norm(tf.random_normal((nb_rows, nb_columns), dtype=dtype), axis=1)\n",
    "    elif scaling == 1:\n",
    "        multiplier = math.sqrt((float(nb_columns))) * tf.ones((nb_rows,), dtype=dtype)\n",
    "    else:\n",
    "        raise ValueError(f'Invalid scaling {scaling}')\n",
    "\n",
    "    return tf.matmul(tf.diag(multiplier), final_matrix)\n",
    "\n",
    "\n",
    "# for bidirectional/masked language modelling\n",
    "def linear_attention(q, k, v):\n",
    "    context = tf.einsum('...nd,...ne->...de', k, v)\n",
    "    out = tf.einsum('...de,...nd->...ne', context, q)\n",
    "    return out\n",
    "\n",
    "# # for unidirectional/causal modelling\n",
    "# def causal_linear_attention(q, k, v):\n",
    "#     k_cumsum = tf.cumsum(k, axis=-2)\n",
    "#     context = tf.einsum('...nd,...ne->...nde', k, v)\n",
    "#     context = tf.cumsum(context, axis=-3)\n",
    "#     context /= tf.expand_dims(k_cumsum, axis=-1)\n",
    "#     out = tf.einsum('...nde,...nd->...ne', context, q)\n",
    "#     return out\n",
    "\n",
    "\n",
    "\n",
    "# elems = np.array([1, 0, 0, 0, 0, 0])\n",
    "# initializer = (np.array(0), np.array(1))\n",
    "# fibonaccis = scan(lambda a, _: (a[1], a[0] + a[1]), elems, initializer)\n",
    "# # fibonaccis == ([1, 1, 2, 3, 5, 8], [1, 2, 3, 5, 8, 13])\n",
    "        \n",
    "def causal_linear_attention(qs, ks, vs): #[bs, num_heads, len, head_dims]\n",
    "    \n",
    "    qs = tf.transpose(qs, (2, 0, 1, 3))\n",
    "    ks = tf.transpose(ks, (2, 0, 1, 3))\n",
    "    vs = tf.transpose(vs, (2, 0, 1, 3))\n",
    "    z_slice_shape = (ks.shape[1], ks.shape[2], ks.shape[-1], vs.shape[-1])\n",
    "    def body(p, qkv):\n",
    "        (q, k, v) = qkv\n",
    "        tmp= tf.einsum('...m,...d->...md', k, v)\n",
    "        tmp_p = p[0] + tmp\n",
    "        X_slice = tf.einsum('...m,...md->...d', q, tmp_p)\n",
    "        return tmp_p, X_slice\n",
    "    init_value = (tf.zeros(z_slice_shape, dtype=qs.dtype), tf.zeros(vs.shape[1:], dtype=qs.dtype))\n",
    "    p, W = tf.scan(body, (qs, ks, vs), init_value) \n",
    "    return tf.transpose(W, (1,2,0,3))  # [bs, num_heads, len, head_dims]\n",
    "\n",
    "def _denominator(qs, ks):\n",
    "    # [bs, num_heads, len, head_dims] -> [len, bs, num_heads, head_dim]\n",
    "    qs = tf.transpose(qs, (2, 0, 1, 3))\n",
    "    ks = tf.transpose(ks, (2, 0, 1, 3))\n",
    "    qs_shape = shape_list(qs)\n",
    "    t_slice_shape = qs_shape[1:]    # (bs, num_heads, head_dim)\n",
    "    res_shape = qs_shape[1:-1]\n",
    "    def body(p, qk):\n",
    "        q, k = qk\n",
    "        tmp = p[0] + k\n",
    "        x = tf.einsum('...m,...m->...', q, tmp)\n",
    "        return tmp, x\n",
    "\n",
    "    init_value = (tf.zeros(t_slice_shape, dtype=qs.dtype),\n",
    "                  tf.zeros(res_shape, dtype=qs.dtype))\n",
    "    p, R = tf.scan(body, (qs, ks), init_value) # R: (len, bs, num_heads)\n",
    "    return tf.transpose(R, (1,2,0))\n",
    "\n",
    "def fast_attention(q, k, v,\n",
    "                   dim_heads,\n",
    "                   nb_features=128,\n",
    "                   redraw_projection=False,\n",
    "                   ortho_scaling=0,\n",
    "                   lm_type='bi',  # unibi, bi, plm\n",
    "                   out_proj_mat=True,\n",
    "                   renormalize_attention=True,\n",
    "                   numerical_stabilizer=1e-6):\n",
    "    q = tf.transpose(q, (0,2,1,3))\n",
    "    k = tf.transpose(k, (0,2,1,3))\n",
    "    v = tf.transpose(v, (0,2,1,3))\n",
    "    q = tf.saturate_cast(q, tf.float32)\n",
    "    k = tf.saturate_cast(k, tf.float32)\n",
    "    v = tf.saturate_cast(v, tf.float32)\n",
    "    res_value = []\n",
    "    if redraw_projection:\n",
    "        # random gaussian orthogonal random matrix for every training iteration\n",
    "        projection_matrix = gaussian_orthogonal_random_matrix(nb_rows=nb_features,\n",
    "                                                              nb_columns=dim_heads,\n",
    "                                                              scaling=ortho_scaling,\n",
    "                                                              dtype=q.dtype)\n",
    "        # print(\"redraw\")\n",
    "    else:\n",
    "#         # fixed gaussian orthogonal random matrix for every training iteration\n",
    "#         projection_matrix = np_gaussian_orthogonal_random_matrix(nb_rows=nb_features,\n",
    "#                                                                  nb_columns=dim_heads,\n",
    "#                                                                  scaling=ortho_scaling,\n",
    "#                                                                  dtype=q.dtype)\n",
    "        # print(\"no-redraw\")\n",
    "        projection_matrix = tf.convert_to_tensor(onp.array(proj_mat), dtype=q.dtype)\n",
    "    create_kernel = partial(nonnegative_softmax_kernel_feature_creator,\n",
    "                            projection_matrix=projection_matrix, eps=numerical_stabilizer)\n",
    "    q_prime = create_kernel(q, is_query=True) # [bs, num_heads, len, head_dims]\n",
    "    k_prime = create_kernel(k, is_query=False)\n",
    "#     q_prime = q\n",
    "#     k_prime = k\n",
    "    res_value.append(q_prime)\n",
    "    res_value.append(k_prime)\n",
    "    if lm_type == 'bi':\n",
    "        out = linear_attention(q_prime, k_prime, v)\n",
    "        res_value.append(out)\n",
    "        if not renormalize_attention:\n",
    "            out = tf.transpose(out, (0, 2, 1, 3))\n",
    "            if out_proj_mat:\n",
    "                return (out, projection_matrix), \n",
    "            else:\n",
    "                return out\n",
    "        else:\n",
    "            # Construct T = (K^{'})^{T} 1_L\n",
    "            T = tf.reduce_sum(k_prime, axis=2,\n",
    "                              keep_dims=False)  # [bs, num_heads, len, head_dims] -> [bs, num_heads, head_dims]\n",
    "            # Construct partition function: R = Q^{'} T = Q^{'}(K^{'})^{T} 1_L\n",
    "            R = tf.einsum('...nd,...d->...n', q_prime, T)\n",
    "    elif lm_type == 'unibi':\n",
    "        out = causal_linear_attention(q_prime, k_prime, v)\n",
    "        if not renormalize_attention:\n",
    "            out = tf.transpose(out, (0, 2, 1, 3))\n",
    "            res_value.append(out)\n",
    "            if out_proj_mat:\n",
    "                return (out, projection_matrix), res_value\n",
    "            else:\n",
    "                return out\n",
    "        else:\n",
    "            R = _denominator(q_prime, k_prime)\n",
    "\n",
    "    elif lm_type == 'plm':\n",
    "        NotImplementedError(\"Need to implement\")\n",
    "    R_shape = shape_list(R)\n",
    "    R_zero_mask = tf.zeros(R_shape, dtype=R.dtype)\n",
    "    R_numerical_stabilizer_mask = R_zero_mask + 2*numerical_stabilizer\n",
    "    # R_add_numerical_stabilizer = tf.where(tf.abs(R) <= numerical_stabilizer, 2*numerical_stabilizer, 0.)\n",
    "    R_add_numerical_stabilizer = tf.where(tf.abs(R) <= numerical_stabilizer, R_numerical_stabilizer_mask, R_zero_mask)\n",
    "    R = R + R_add_numerical_stabilizer\n",
    "    R = tf.expand_dims(tf.reciprocal(R), axis=-1) # [bs, num_heads, len] -> [bs, num_heads, len, 1]\n",
    "    out = out * R\n",
    "    # [bs, num_heads, len, head_dims]\n",
    "    out = tf.transpose(out, (0, 2, 1, 3))\n",
    "    if out_proj_mat:\n",
    "        return (out, projection_matrix), res_value\n",
    "    else:\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.InteractiveSession at 0x7fccbd367b38>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_tf = tf.constant(value=query)\n",
    "k_tf = tf.constant(value=key)\n",
    "v_tf = tf.constant(value=value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1022 17:21:52.859043 140518241646336 deprecation.py:506] From <ipython-input-24-eea919aa8e2a>:100: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "W1022 17:21:53.058085 140518241646336 deprecation.py:323] From <ipython-input-24-eea919aa8e2a>:328: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# (res4, proj_mat_tf), res_value_tf = fast_attention(q_tf, k_tf, v_tf, \n",
    "#                                                    dim_heads=64, \n",
    "#                                                    nb_features=256, \n",
    "#                                                    renormalize_attention=False, \n",
    "#                                                    lm_type='unibi')\n",
    "(res4, proj_mat_tf), res_value_tf = fast_attention(q_tf, k_tf, v_tf, \n",
    "                                                   dim_heads=64, \n",
    "                                                   nb_features=128, \n",
    "                                                   renormalize_attention=True, \n",
    "                                                   lm_type='unibi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_mat_tf = proj_mat_tf.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "res4 = res4.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float32'), dtype('float32'))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1.dtype, res4.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1244449638070364e-07"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onp.sum(onp.abs(res1-res4))/onp.prod(res1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.97466600e-01, 9.62948263e-01, 3.28020155e-01, 7.86812007e-01,\n",
       "       1.13687376e-02, 7.32347190e-01, 4.09603089e-01, 2.03831524e-01,\n",
       "       9.48454618e-01, 7.13827968e-01, 3.75037044e-01, 9.04710144e-02,\n",
       "       3.70306194e-01, 8.71309459e-01, 8.32820907e-02, 1.18514225e-01,\n",
       "       6.50837302e-01, 2.13438377e-01, 6.52033925e-01, 5.96800186e-02,\n",
       "       1.95140187e-02, 6.87420309e-01, 3.46510410e-02, 1.03139736e-01,\n",
       "       9.99977529e-01, 3.40717077e-01, 3.94634336e-01, 2.42169291e-01,\n",
       "       9.89044845e-01, 7.46673346e-01, 9.13960099e-01, 7.44454682e-01,\n",
       "       1.49573207e-01, 1.02939233e-01, 5.94359398e-01, 7.78720737e-01,\n",
       "       8.78095746e-01, 9.02316332e-01, 8.75927925e-01, 4.81646687e-01,\n",
       "       7.19547272e-02, 9.09988463e-01, 7.70958126e-01, 7.17091203e-01,\n",
       "       6.88990712e-01, 4.43742603e-01, 4.21752967e-03, 6.18094742e-01,\n",
       "       1.69340968e-01, 7.01120079e-01, 3.20408314e-01, 2.86291353e-04,\n",
       "       8.27001095e-01, 5.56121111e-01, 3.66792798e-01, 8.09863091e-01,\n",
       "       3.88153702e-01, 4.83987033e-01, 5.48487723e-01, 3.34746599e-01,\n",
       "       2.87011057e-01, 8.01066160e-01, 6.29976869e-01, 5.99951327e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res4[0,0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([8.97466660e-01, 9.62948263e-01, 3.28020155e-01,\n",
       "             7.86811888e-01, 1.13687385e-02, 7.32347071e-01,\n",
       "             4.09603208e-01, 2.03831583e-01, 9.48454559e-01,\n",
       "             7.13828027e-01, 3.75037014e-01, 9.04710144e-02,\n",
       "             3.70306134e-01, 8.71309638e-01, 8.32820833e-02,\n",
       "             1.18514203e-01, 6.50837302e-01, 2.13438451e-01,\n",
       "             6.52033806e-01, 5.96800074e-02, 1.95140168e-02,\n",
       "             6.87420130e-01, 3.46510336e-02, 1.03139766e-01,\n",
       "             9.99977529e-01, 3.40716928e-01, 3.94634247e-01,\n",
       "             2.42169246e-01, 9.89044607e-01, 7.46673405e-01,\n",
       "             9.13960218e-01, 7.44454384e-01, 1.49573132e-01,\n",
       "             1.02939256e-01, 5.94359696e-01, 7.78720438e-01,\n",
       "             8.78095865e-01, 9.02316213e-01, 8.75927627e-01,\n",
       "             4.81646538e-01, 7.19547570e-02, 9.09988523e-01,\n",
       "             7.70958126e-01, 7.17091262e-01, 6.88990593e-01,\n",
       "             4.43742752e-01, 4.21752967e-03, 6.18094563e-01,\n",
       "             1.69340938e-01, 7.01120257e-01, 3.20408314e-01,\n",
       "             2.86291441e-04, 8.27001095e-01, 5.56121111e-01,\n",
       "             3.66792798e-01, 8.09863031e-01, 3.88153642e-01,\n",
       "             4.83987153e-01, 5.48487723e-01, 3.34746569e-01,\n",
       "             2.87011147e-01, 8.01066041e-01, 6.29976690e-01,\n",
       "             5.99951506e-01], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1[0,0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onp.sum(onp.abs(proj_mat-proj_mat_tf))/onp.prod(proj_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-6590f7283050>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mq_prime_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_prime_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres_value_tf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mq_prime_o\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_prime_o\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_o\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres_vlaue_orig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "q_prime_tf, k_prime_tf, out_tf = [it.eval() for it in res_value_tf]\n",
    "q_prime_o, k_prime_o, out_o = res_vlaue_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff2mat(a, b):\n",
    "    return onp.sum(onp.abs(a-b))/onp.prod(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.401014523455984e-11"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff2mat(q_prime_tf, q_prime_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.98882617644814e-12"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff2mat(k_prime_tf, k_prime_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10013184696435928"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff2mat(out_tf.transpose(0,2,1,3), out_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sub got incompatible shapes for broadcasting: (2, 512, 16, 64), (2, 16, 512, 64).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-b975143a7dcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdiff2mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_o\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-54e05e7a7bdf>\u001b[0m in \u001b[0;36mdiff2mat\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdiff2mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0monp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0monp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mdeferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   4806\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_scalar_types\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_arraylike_types\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTracer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4807\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4808\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4809\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdeferring_binary_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   4810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4811\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_swap_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4812\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4814\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_unimplemented_setitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlax_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_promote_args_inexact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m     \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlax_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_promote_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlax_doc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dedent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlax_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m   \u001b[0;34mr\"\"\"Elementwise subtraction: :math:`x - y`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msub_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0mtop_trace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_top_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0mtracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfull_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mapply_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m   \u001b[0;34m\"\"\"Impl rule that compiles and runs a single primitive 'prim' using XLA.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m   \u001b[0mcompiled_fun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxla_primitive_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0munsafe_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36mxla_primitive_callable\u001b[0;34m(prim, *arg_specs, **params)\u001b[0m\n\u001b[1;32m    256\u001b[0m     return _xla_callable(lu.wrap_init(prim_fun), device, None, \"prim\", donated_invars,\n\u001b[1;32m    257\u001b[0m                          *arg_specs)\n\u001b[0;32m--> 258\u001b[0;31m   \u001b[0maval_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstract_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mavals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mprim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0mhandle_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maval_to_result_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maval_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36mstandard_abstract_eval\u001b[0;34m(prim, shape_rule, dtype_rule, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1938\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mConcreteArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mleast_specialized\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mShapedArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1940\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mShapedArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape_rule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype_rule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1941\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mleast_specialized\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mUnshapedArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mUnshapedArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype_rule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_broadcasting_shape_rule\u001b[0;34m(name, *avals)\u001b[0m\n\u001b[1;32m   2003\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mresult_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2004\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{} got incompatible shapes for broadcasting: {}.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2005\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2006\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: sub got incompatible shapes for broadcasting: (2, 512, 16, 64), (2, 16, 512, 64)."
     ]
    }
   ],
   "source": [
    "diff2mat(out_tf, out_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16.50806046  0.          0.          0.          0.        ]\n",
      " [16.29816246 17.48893929  0.          0.          0.        ]\n",
      " [15.91574478 16.28504562 16.20175552  0.          0.        ]\n",
      " [15.5081892  16.25076866 15.39020538 12.91930771  0.        ]\n",
      " [15.7975235  17.56301689 17.49247932 14.07947159 17.42897034]]\n",
      "(2, 16, 512, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.48154339e+01, 1.58964101e+01, 5.41497754e+00, 1.29887412e+01,\n",
       "       1.87675838e-01, 1.20896337e+01, 6.76175305e+00, 3.36486385e+00,\n",
       "       1.56571481e+01, 1.17839172e+01, 6.19113518e+00, 1.49350097e+00,\n",
       "       6.11303654e+00, 1.43836302e+01, 1.37482603e+00, 1.95643999e+00,\n",
       "       1.07440615e+01, 3.52345412e+00, 1.07638145e+01, 9.85201355e-01,\n",
       "       3.22138631e-01, 1.13479770e+01, 5.72021480e-01, 1.70263712e+00,\n",
       "       1.65076895e+01, 5.62457860e+00, 6.51464797e+00, 3.99774529e+00,\n",
       "       1.63272141e+01, 1.23261317e+01, 1.50877096e+01, 1.22895039e+01,\n",
       "       2.46916379e+00, 1.69932733e+00, 9.81172284e+00, 1.28551700e+01,\n",
       "       1.44956596e+01, 1.48954926e+01, 1.44598721e+01, 7.95105262e+00,\n",
       "       1.18783323e+00, 1.50221455e+01, 1.27270253e+01, 1.18377859e+01,\n",
       "       1.13739013e+01, 7.32533119e+00, 6.96232348e-02, 1.02035464e+01,\n",
       "       2.79549119e+00, 1.15741336e+01, 5.28932032e+00, 4.72611545e-03,\n",
       "       1.36521870e+01, 9.18048190e+00, 6.05503818e+00, 1.33692698e+01,\n",
       "       6.40766576e+00, 7.98968819e+00, 9.05446947e+00, 5.52601759e+00,\n",
       "       4.73799637e+00, 1.32240486e+01, 1.03996972e+01, 9.90403376e+00])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(2, 512, 16, 64)\n",
    "q = query.transpose(0, 2, 1, 3) \n",
    "k = key.transpose(0, 2, 1, 3)\n",
    "v = value.transpose(0, 2, 1, 3)\n",
    "attn_score = onp.matmul(q, k.transpose(0, 1, 3, 2))\n",
    "low_triangular_mask = onp.tril(onp.ones(attn_score.shape[-2:]), 0)\n",
    "attn_score = attn_score * low_triangular_mask[None, None, ...]\n",
    "print(attn_score[0,0, :5, :5])\n",
    "context_layer = onp.matmul(attn_score, v)\n",
    "print(context_layer.shape)\n",
    "context_layer[0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4042.66284755, 3990.22515727, 3974.57602455, 3950.01565792,\n",
       "       4034.0444153 , 3949.22006486, 4063.28383501, 4086.79502522,\n",
       "       4060.83035086, 4231.00437254, 3835.17604203, 4057.66794557,\n",
       "       4014.59497161, 3989.75054187, 3957.32204225, 4092.65127314,\n",
       "       3965.08366841, 3985.22160199, 3889.88242935, 3928.79081328,\n",
       "       4015.04298703, 3921.49873215, 3770.73859598, 4088.15464409,\n",
       "       3950.66828529, 3935.88696788, 4078.42867247, 3910.79636011,\n",
       "       3760.36368068, 3900.10490351, 3719.35855678, 3869.55099071,\n",
       "       4058.49856933, 4194.3131201 , 3959.01618253, 3715.26492307,\n",
       "       3927.54794701, 4039.79429695, 3975.83636138, 3828.5063419 ,\n",
       "       4011.38153009, 4035.81040517, 4033.60301051, 3925.67112959,\n",
       "       3922.30926799, 3970.74214615, 3900.81017108, 3903.0843065 ,\n",
       "       3834.8749698 , 3901.90958478, 4017.29870386, 3961.33044824,\n",
       "       3949.53327435, 4073.40464346, 3864.3495142 , 3995.36130589,\n",
       "       4026.97116444, 3858.34154333, 4085.55536482, 3887.37478691,\n",
       "       4076.2165572 , 4052.30418027, 4057.83999857, 3972.15822594])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_layer[0,0,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.48154356e+01, 1.58964120e+01, 5.41497821e+00, 1.29887427e+01,\n",
       "       1.87675860e-01, 1.20896351e+01, 6.76175387e+00, 3.36486427e+00,\n",
       "       1.56571500e+01, 1.17839186e+01, 6.19113590e+00, 1.49350115e+00,\n",
       "       6.11303727e+00, 1.43836318e+01, 1.37482621e+00, 1.95644022e+00,\n",
       "       1.07440628e+01, 3.52345451e+00, 1.07638158e+01, 9.85201469e-01,\n",
       "       3.22138669e-01, 1.13479784e+01, 5.72021547e-01, 1.70263732e+00,\n",
       "       1.65076915e+01, 5.62457929e+00, 6.51464874e+00, 3.99774576e+00,\n",
       "       1.63272161e+01, 1.23261332e+01, 1.50877113e+01, 1.22895053e+01,\n",
       "       2.46916407e+00, 1.69932753e+00, 9.81172403e+00, 1.28551714e+01,\n",
       "       1.44956614e+01, 1.48954943e+01, 1.44598738e+01, 7.95105355e+00,\n",
       "       1.18783337e+00, 1.50221472e+01, 1.27270268e+01, 1.18377873e+01,\n",
       "       1.13739027e+01, 7.32533205e+00, 6.96232432e-02, 1.02035476e+01,\n",
       "       2.79549152e+00, 1.15741350e+01, 5.28932092e+00, 4.72611600e-03,\n",
       "       1.36521886e+01, 9.18048300e+00, 6.05503895e+00, 1.33692715e+01,\n",
       "       6.40766653e+00, 7.98968911e+00, 9.05447059e+00, 5.52601822e+00,\n",
       "       4.73799692e+00, 1.32240501e+01, 1.03996985e+01, 9.90403496e+00])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.48154364e+01, 1.58964128e+01, 5.41497850e+00, 1.29887409e+01,\n",
       "       1.87675878e-01, 1.20896330e+01, 6.76175356e+00, 3.36486411e+00,\n",
       "       1.56571512e+01, 1.17839212e+01, 6.19113684e+00, 1.49350107e+00,\n",
       "       6.11303616e+00, 1.43836317e+01, 1.37482619e+00, 1.95643985e+00,\n",
       "       1.07440615e+01, 3.52345443e+00, 1.07638149e+01, 9.85201359e-01,\n",
       "       3.22138667e-01, 1.13479776e+01, 5.72021544e-01, 1.70263779e+00,\n",
       "       1.65076923e+01, 5.62457895e+00, 6.51464748e+00, 3.99774575e+00,\n",
       "       1.63272152e+01, 1.23261328e+01, 1.50877132e+01, 1.22895060e+01,\n",
       "       2.46916413e+00, 1.69932783e+00, 9.81172276e+00, 1.28551741e+01,\n",
       "       1.44956608e+01, 1.48954926e+01, 1.44598742e+01, 7.95105410e+00,\n",
       "       1.18783343e+00, 1.50221472e+01, 1.27270260e+01, 1.18377876e+01,\n",
       "       1.13739014e+01, 7.32533264e+00, 6.96232542e-02, 1.02035475e+01,\n",
       "       2.79549217e+00, 1.15741358e+01, 5.28932047e+00, 4.72611655e-03,\n",
       "       1.36521912e+01, 9.18048382e+00, 6.05503845e+00, 1.33692703e+01,\n",
       "       6.40766621e+00, 7.98969030e+00, 9.05447102e+00, 5.52601814e+00,\n",
       "       4.73799706e+00, 1.32240486e+01, 1.03996992e+01, 9.90403557e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2[0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 16, 512, 64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=[]\n",
    "q = query.transpose(1, 0, 2, 3)\n",
    "k = key.transpose(1, 0, 2, 3)\n",
    "v = value.transpose(1, 0, 2, 3)\n",
    "p = onp.zeros((2, 16, 64, 64))\n",
    "for i in range(q.shape[0]):\n",
    "    tmp = onp.einsum('...m,...d->...md', k[i], v[i])\n",
    "    p = p + tmp\n",
    "    out = onp.einsum('...m,...md->...d', q[i], p)\n",
    "    res.append(out[None,...])\n",
    "res = onp.concatenate(res, 0).transpose(1,2,0,3)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _numerator_fwd(z_slice_shape, precision, qs, ks, vs):\n",
    "  def body(p, qkv):\n",
    "    (q, k, v) = qkv\n",
    "    tmp= jnp.einsum('...m,...d->...md', k, v, precision=precision)\n",
    "    p = p + tmp\n",
    "    print(\"body p shape: \", p.shape) #(2, 16, 256, 64)\n",
    "    print(\"body tmp shape: \", tmp.shape) #(2, 16, 256, 64)\n",
    "    print(\"body q shape: \", q.shape) #(2, 16, 256)\n",
    "    X_slice = jnp.einsum('...m,...md->...d', q, p, precision=precision)\n",
    "    print(\"body X_slice shape: \", X_slice.shape) #(2, 16, 64)\n",
    "    return p, X_slice\n",
    "  init_value = jnp.zeros(z_slice_shape)\n",
    "  print(\"body qs shape: \", qs.shape)\n",
    "  p, W = lax.scan(body, init_value, (qs, ks, vs))\n",
    "  return W, (p, qs, ks, vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(2, 512, 16, 64)\n",
    "q = query.transpose(0, 2, 1, 3) \n",
    "k = key.transpose(0, 2, 1, 3)\n",
    "v = value.transpose(0, 2, 1, 3)\n",
    "def causal_linear_attention(q, k, v): # (bs, num_head, len, head_dim)\n",
    "#     k_cumsum = onp.cumsum(k, axis=-2)\n",
    "    context = onp.einsum('...nd,...ne->...nde', k, v)\n",
    "    context = onp.cumsum(context, axis=-3)\n",
    "#     context /= onp.expand_dims(k_cumsum, axis=-1)\n",
    "    out = onp.einsum('...nd,...nde->...ne', q, context)\n",
    "    return out\n",
    "res2 = causal_linear_attention(q,k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 16, 512, 64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
